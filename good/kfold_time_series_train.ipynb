{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c68948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pydicom\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import scipy.ndimage\n",
    "from skimage import morphology\n",
    "from skimage import measure\n",
    "from skimage.transform import resize\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import filters\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Sequential \n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import Conv2D, MultiHeadAttention, AveragePooling2D, GlobalAveragePooling2D, MaxPooling2D , Conv3D, Layer, MaxPooling2D, Dropout, Flatten, Dense, GRU, ConvLSTM2D, Input, BatchNormalization, TimeDistributed, MaxPooling3D, Bidirectional, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from random import shuffle\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "import pydicom\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "# import bisect\n",
    "np.random.seed(1234)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "MAX_SEQ_LENGTH = 200\n",
    "NUM_FEATURES = 1024\n",
    "IMG_SIZE = 512\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 2\n",
    "NUM_SCANS = 8\n",
    "NUM_CHANNELS = 1\n",
    "INPUT_DIM = 256\n",
    "\n",
    "keras.utils.set_random_seed(1)\n",
    "random.seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d208901",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "all_ids = pd.read_csv('all_ids_updated.csv')\n",
    "all_ids.ycoord = all_ids.ycoord.replace('True', '1.0').astype('float')\n",
    "all_ids = all_ids.drop_duplicates('StudyInstanceUID')\n",
    "fold_df = pd.read_csv('folds.csv')\n",
    "\n",
    "directory = '/home/shared/nps/coat_np_0' + str(fold) + '/'\n",
    "\n",
    "lisdir = os.listdir(directory)\n",
    "print(len(lisdir))\n",
    "\n",
    "files = pd.DataFrame({'file_name':lisdir})\n",
    "files['StudyInstanceUID'] = files['file_name'].str.replace('.npy','')\n",
    "files = pd.merge(files, all_ids)\n",
    "files = files.drop(columns=['SeriesInstanceUID', 'SOPInstanceUID',\n",
    "       'pe_present_on_image','ycoord', 'contains_lung'])\n",
    "files = pd.merge(files, fold_df)\n",
    "files = files[files.indeterminate == False].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b7f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 64\n",
    "MAX_SEQ_LENGTH = 200\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    \"\"\"Generates data for Keras\n",
    "    Sequence based data generator. Suitable for building data generator for training and prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, list_IDs, study_ids, num_features, directory,\n",
    "                 seq_length, return_type = 'both', to_fit=True, batch_size=32, \n",
    "                 shuffle=True, full_set = None, random=False, set_type='test',\n",
    "                 feat_type = 'feats', oversample=True, crop=False ):\n",
    "\n",
    "        self.list_IDs = list_IDs\n",
    "        self.study_ids = study_ids\n",
    "        self.to_fit = to_fit\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_features = num_features\n",
    "        self.seq_length = seq_length\n",
    "        self.full_set = full_set\n",
    "        self.return_type = return_type\n",
    "        self.random = random\n",
    "        warnings.filterwarnings(action='ignore')\n",
    "        self.directory = directory\n",
    "        self.set_type = set_type\n",
    "        self.feat_type = feat_type\n",
    "        self.oversample=oversample\n",
    "        self.crop= crop\n",
    "        \n",
    "        if self.set_type == 'val':\n",
    "            sample_pos = self.full_set[self.full_set.negative_exam_for_pe == False]\n",
    "            sample_neg = self.full_set[self.full_set.negative_exam_for_pe == True]\n",
    "            self.study_ids = pd.concat([sample_pos, sample_neg.sample(n=len(sample_pos))]).sample(frac = 1).reset_index(drop=True)\n",
    "            self.list_IDs = np.arange(0, len(self.study_ids))\n",
    "        self.on_epoch_end()\n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\n",
    "        :return: number of batches per epoch\n",
    "        \"\"\"\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "#         print('starting')\n",
    "        indexes = self.list_IDs[index * self.batch_size:((index+1) * self.batch_size)]\n",
    "        x_feats = np.zeros([self.batch_size, self.seq_length, self.num_features])\n",
    "        y_seq = np.zeros([self.batch_size, self.seq_length, 1])\n",
    "        y_tot = np.zeros([self.batch_size, 1])\n",
    "        for i in range(0,self.batch_size):\n",
    "            x, y = self._get_scan_data(self.study_ids.iloc[indexes[i]].StudyInstanceUID)\n",
    "            x_feats[i] = x\n",
    "            y_seq[i] = y[0]\n",
    "            y_tot[i] = y[1]\n",
    "            \n",
    "        if self.to_fit:\n",
    "            if self.return_type =='both':\n",
    "                y = [np.array(y_seq), y_tot]\n",
    "            elif self.return_type == 'seq':\n",
    "                y = y_seq\n",
    "            elif self.return_type == 'tot':\n",
    "                y = y_tot\n",
    "            else:\n",
    "                print('valid return types are both, seq and tot')\n",
    "                return False\n",
    "            x = x_feats\n",
    "            return x, y\n",
    "        else:\n",
    "            return (X)\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\n",
    "        \"\"\"\n",
    "        if self.set_type == 'train':\n",
    "            sample_pos = self.full_set[self.full_set.negative_exam_for_pe == False]\n",
    "            sample_neg = self.full_set[self.full_set.negative_exam_for_pe == True]\n",
    "            if self.oversample == False:\n",
    "                self.study_ids = pd.concat([sample_pos, sample_neg.sample(n=len(sample_pos))]).sample(frac=1).reset_index(drop=True)\n",
    "            else:\n",
    "                self.study_ids = pd.concat([sample_pos.sample(n=len(sample_neg), replace=True), sample_neg]).sample(frac = 1).reset_index(drop=True)\n",
    "            self.list_IDs = np.arange(0, len(self.study_ids))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.list_IDs)\n",
    "            \n",
    "    def _get_scan_data(self, study_id):\n",
    "        scan = pd.DataFrame(np.load(self.directory +study_id + '.npy', allow_pickle=True).tolist())\n",
    "        scan.ycoord = scan.ycoord.replace('True', '1.0').astype('float')\n",
    "        scan = scan.sort_values(by=['ycoord'], ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        features = scan.features.tolist()\n",
    "        seq = scan.pe_present_on_image.tolist()\n",
    "        seq = np.reshape(seq, [len(seq),1])\n",
    "        tot = self.study_ids[self.study_ids.StudyInstanceUID == study_id].negative_exam_for_pe.iloc[0]\n",
    "        \n",
    "        if self.set_type == 'train' and self.random == True:\n",
    "            new_len = int(len(scan) * (random.random()*0.25 +0.9))\n",
    "            inst = np.round(np.arange(0,new_len)/new_len*len(features)).astype(int)\n",
    "            features = (np.array(features)[inst]).tolist()\n",
    "            seq = seq[inst]   \n",
    "            \n",
    "            new_len = int(len(features)*(1 - np.random.random() * 0.05))\n",
    "            new_start = int(np.random.random() * (len(features)- new_len))\n",
    "            features = features[new_start:new_start+new_len]\n",
    "            seq = seq[new_start:new_start+new_len]        \n",
    "        \n",
    "        \n",
    "        if len(features)>=self.seq_length:\n",
    "            if self.crop==False:\n",
    "                inst = np.round(np.arange(0,self.seq_length)/self.seq_length*len(features)).astype(int)\n",
    "                xs = (np.array(features)[inst]).tolist()\n",
    "                ys = seq[inst]\n",
    "            if self.crop==True:\n",
    "                xs = (np.array(features)[:self.seq_length]).tolist()\n",
    "                ys = seq[:self.seq_length]\n",
    "        else:\n",
    "            xs = np.zeros([self.seq_length, self.num_features])\n",
    "            ys = np.zeros([self.seq_length, 1])\n",
    "\n",
    "            start = int(np.floor((self.seq_length - len(features))/2))\n",
    "            end = start + len(features)\n",
    "            xs[start:end] = features\n",
    "            ys[start:end] = seq\n",
    "\n",
    "        ys = ys.tolist()\n",
    "                    \n",
    "        return (xs, [ys, tot])\n",
    "\n",
    "    \n",
    "def get_generators(df, fold, batch_size, out, feat, random, seq_len=200, crop=True, np_dir='/home/shared/coat_np_0', osa=True, test_only=False):\n",
    "\n",
    "    dire = np_dir + str(fold) + '/'\n",
    "\n",
    "    if test_only:\n",
    "        return DataGenerator(np.arange(0, len(df)),\n",
    "                                    df, \n",
    "                                    64,\n",
    "                                    dire,\n",
    "                                    seq_len,\n",
    "                                    batch_size=batch_size,\n",
    "                                    set_type = 'test',\n",
    "                                    return_type=out,\n",
    "                                    feat_type=feat,\n",
    "                                    crop=crop,\n",
    "                                    shuffle=False)\n",
    "    \n",
    "    test_df = df[df.fold == fold]\n",
    "    train_df = df[df.fold != fold]\n",
    "        \n",
    "    print(train_df.fold.unique())\n",
    "    print(test_df.fold.unique())\n",
    "    test_generator = DataGenerator(np.arange(0, len(test_df)),\n",
    "                                    test_df, \n",
    "                                    64,\n",
    "                                    dire,\n",
    "                                    seq_len,\n",
    "                                    batch_size=batch_size,\n",
    "                                    set_type = 'test',\n",
    "                                    return_type=out,\n",
    "                                    feat_type=feat,\n",
    "                                    crop=crop)\n",
    "    \n",
    "    val_generator = DataGenerator(np.arange(0, len(test_df)),\n",
    "                                  test_df, \n",
    "                                  64,\n",
    "                                  dire,\n",
    "                                  seq_len,\n",
    "                                  batch_size=batch_size,\n",
    "                                  set_type ='val',\n",
    "                                  full_set=test_df,\n",
    "                                  return_type=out,\n",
    "                                  feat_type=feat,\n",
    "                                  crop=crop)\n",
    "    \n",
    "    train_generator = DataGenerator(np.arange(0, len(train_df)),\n",
    "                                  train_df, \n",
    "                                  64,\n",
    "                                  dire,\n",
    "                                  seq_len,\n",
    "                                  batch_size=batch_size,\n",
    "                                  set_type ='train',\n",
    "                                  full_set = train_df,\n",
    "                                  random = random,\n",
    "                                  return_type=out,\n",
    "                                  feat_type=feat,\n",
    "                                  crop=crop,\n",
    "                                  oversample=osa)\n",
    "    \n",
    "    return train_generator, val_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c54eeaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_attn_lstm(seq_len, noise=0.001):\n",
    "        num_heads = 1\n",
    "        key_dim = 8\n",
    "        inputs = keras.Input(shape=(seq_len, 64))\n",
    "        x = layers.BatchNormalization()(inputs)\n",
    "        x = layers.GaussianNoise(noise)(x)\n",
    "        x = layers.TimeDistributed(layers.Dropout(0.3))(x)\n",
    "        attn = layers.MultiHeadAttention(num_heads=num_heads,\n",
    "                                                     key_dim=key_dim, \n",
    "                                                     dropout=0.4\n",
    "                                                    )(x, x)\n",
    "        attn = layers.LayerNormalization()(attn)\n",
    "        x = layers.Add()((x,attn))\n",
    "        x = layers.TimeDistributed(layers.Dropout(0.4))(x)\n",
    "        x = layers.Bidirectional(layers.LSTM(units=8, \n",
    "                                             return_sequences=True,\n",
    "                                             dropout=0.3, \n",
    "                                             recurrent_regularizer=keras.regularizers.L2(0.01),\n",
    "                                            ))(x)\n",
    "        x = layers.TimeDistributed(layers.Dropout(0.4))(x)\n",
    "        x = layers.LSTM(units=8, return_sequences=True,\n",
    "                        dropout=0.3, \n",
    "#                         bias_regularizer=keras.regularizers.L2(0.005)\n",
    "                       )(x)\n",
    "        x = layers.TimeDistributed(layers.Dropout(0.4))(x)\n",
    "        x = layers.Bidirectional(layers.LSTM(units=8,\n",
    "                                             return_sequences=False, \n",
    "                                             dropout=0.3,\n",
    "#                                              bias_regularizer=keras.regularizers.L2(0.005),\n",
    "                                            ))(x)\n",
    "        x = layers.Dropout(0.4)(x)\n",
    "        stack_outputs = layers.Dense(1, activation='sigmoid', name='tot_out')(x)\n",
    "\n",
    "        model = keras.models.Model(inputs=inputs, outputs=stack_outputs)\n",
    "        \n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a912fae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq_len = 208\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "rs = 4 \n",
    "folds = range(0,10)\n",
    "for fold in folds:        \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    keras.utils.set_random_seed(rs)\n",
    "    random.seed(rs)\n",
    "    np.random.seed(rs)\n",
    "\n",
    "    model = get_attn_lstm(seq_len, noise=0.005)\n",
    "\n",
    "    np_dir = '/home/shared/nps/coat_np_0'\n",
    "\n",
    "    batch_size = 32\n",
    "    training_gen, validation_gen, test_gen = get_generators(files, fold, \n",
    "                                                            batch_size, \n",
    "                                                            'tot', \n",
    "                                                            'feats', \n",
    "                                                            True, \n",
    "                                                            seq_len=seq_len, \n",
    "                                                            np_dir=np_dir, \n",
    "                                                            crop=False,\n",
    "                                                            osa=False\n",
    "                                                           )\n",
    "\n",
    "    lr = 0.001\n",
    "    opt = keras.optimizers.Nadam(learning_rate=lr)#,beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "    scheduler = keras.optimizers.schedules.ExponentialDecay(lr, 2000,0.99)\n",
    "    scheduler = keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "    model.compile(\n",
    "            optimizer=opt,\n",
    "            loss=keras.losses.binary_crossentropy,\n",
    "            metrics=[\"accuracy\",]\n",
    "        )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    checkpoint_filepath = '/home/shared/model_checkpoint_paige/scan/reg5-kfold_0'+str(fold)\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        training_gen, \n",
    "        validation_data = validation_gen,\n",
    "        epochs=15,\n",
    "        callbacks=[checkpoint,scheduler,\n",
    "                   es\n",
    "                  ],\n",
    "    )\n",
    "\n",
    "    df_hist = pd.DataFrame(history.history)\n",
    "    df_hist.to_csv('/home/shared/model_checkpoint_paige/scan/hist-reg5-kfold_0'+str(fold)+'.csv')\n",
    "\n",
    "    _,_, test_gen = get_generators(files, fold,  1,  'tot', 'feats', \n",
    "                                   True,  seq_len=seq_len, np_dir=np_dir, \n",
    "                                   crop=False, osa=False)\n",
    "\n",
    "    t_len = test_gen.__len__()\n",
    "    x_test = np.zeros([t_len,208,num_features])\n",
    "    y_test = np.zeros([t_len,1])\n",
    "    for i in range(0,t_len):\n",
    "        x_test[i],y= test_gen.__getitem__(i)\n",
    "        y_test[i] = y\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    y_pred = model(x_test, training=False)\n",
    "\n",
    "    cm = confusion_matrix(y_test, np.asarray(y_pred).round())\n",
    "\n",
    "    sepecifity = cm[1][1]/(cm[1][1]+cm[1][0])\n",
    "    sensitivity = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "    accuracy = (cm[1][1]+cm[0][0])/(cm[1][1]+cm[1][0]+cm[0][0]+cm[0][1])\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    print(cm)\n",
    "    print(sensitivity, sepecifity, accuracy, roc_auc)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbafbbfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq_len=208\n",
    "dos5 = pd.DataFrame(columns=['fold','auc','sensitivity', 'specificity', 'accuracy','npv','ppv'])\n",
    "\n",
    "plt.figure(0).clf()\n",
    "for fold in range(0,10):\n",
    "\n",
    "    model = get_attn_lstm(seq_len)\n",
    "\n",
    "    np_dir = '/home/shared/nps/coat_np_0'\n",
    "\n",
    "    batch_size = 1\n",
    "    _,_, test_gen = get_generators(files, fold, \n",
    "                                   batch_size, \n",
    "                                   'tot', \n",
    "                                   'feats', \n",
    "                                   True, \n",
    "                                   seq_len=seq_len, \n",
    "                                   np_dir=np_dir, \n",
    "                                   crop=False,\n",
    "                                   osa=False)\n",
    "    \n",
    "    \n",
    "    t_len = test_gen.__len__()\n",
    "    x_test = np.zeros([t_len,208,num_features])\n",
    "    y_test = np.zeros([t_len,1])\n",
    "    for i in range(0,t_len):\n",
    "        x_test[i],y= test_gen.__getitem__(i)\n",
    "        y_test[i] = y\n",
    "        \n",
    "    lr = 0.001\n",
    "    opt = keras.optimizers.Nadam(learning_rate=lr)\n",
    "    model.compile(\n",
    "            optimizer=opt,\n",
    "            loss=keras.losses.binary_crossentropy,\n",
    "            metrics=[\"accuracy\",]\n",
    "        )\n",
    "\n",
    "    checkpoint_filepath = '/home/shared/model_checkpoint_paige/scan/reg5-kfold_0'+str(fold)\n",
    "    model.load_weights(checkpoint_filepath).expect_partial()\n",
    "    \n",
    "    (fpr,tpr), data, cm = test_version(model, x_test,y_test)\n",
    "    \n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    plt.plot(fpr,tpr, label='Fold '+str(fold +1)+' AUC={:.4f}'.format(auc))\n",
    "    \n",
    "    dos5 = dos5.append({'fold':fold,'auc':data[0],'sensitivity':data[1], 'specificity':data[2], 'accuracy':data[3], 'npv':data[4],'ppv':data[5]}, ignore_index=True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(dos5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-dsaa",
   "language": "python",
   "name": "tf-gpu-dsaa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
