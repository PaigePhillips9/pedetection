{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd115d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.1\n",
      "2.8.0\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pydicom\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import scipy.ndimage\n",
    "from skimage import morphology\n",
    "from skimage import measure\n",
    "from skimage.transform import resize\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage import filters\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Sequential \n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import Conv2D, MultiHeadAttention, AveragePooling2D, GlobalAveragePooling2D, MaxPooling2D , Conv3D, Layer, MaxPooling2D, Dropout, Flatten, Dense, GRU, ConvLSTM2D, Input, BatchNormalization, TimeDistributed, MaxPooling3D, Bidirectional, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from random import shuffle\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# from tensorflow_docs.vis import embed\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "\n",
    "# import cv2\n",
    "# import imageio\n",
    "# import cv2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import IPython\n",
    "# from six.moves import urllib\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "import pydicom\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "# import bisect\n",
    "np.random.seed(1234)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "MAX_SEQ_LENGTH = 200\n",
    "NUM_FEATURES = 1024\n",
    "IMG_SIZE = 512\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 2\n",
    "NUM_SCANS = 8\n",
    "NUM_CHANNELS = 1\n",
    "INPUT_DIM = 256\n",
    "\n",
    "keras.utils.set_random_seed(1)\n",
    "random.seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dd2400c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3347949/3022398301.py:1: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test = pd.read_csv('test_ids.csv')\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('test_ids.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d208901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650\n",
      "hello?\n",
      "650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3347949/2467093630.py:2: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test = pd.read_csv('test_ids.csv')\n",
      "/tmp/ipykernel_3347949/2467093630.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test.ycoord = test.ycoord.replace('True', '1.0').astype('float')\n",
      "/tmp/ipykernel_3347949/2467093630.py:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  files['StudyInstanceUID'] = files['file_name'].str.replace('.npy','')\n"
     ]
    }
   ],
   "source": [
    "fold = 0\n",
    "test = pd.read_csv('test_ids.csv')\n",
    "test = test[test.ycoord != 'ERROR']\n",
    "test.ycoord = test.ycoord.replace('True', '1.0').astype('float')\n",
    "test = test.drop_duplicates('StudyInstanceUID')\n",
    "print(len(test))\n",
    "directory = '/home/shared/test/coat_np_0' + str(fold) + '/'\n",
    "\n",
    "lisdir = os.listdir(directory)\n",
    "print('hello?')\n",
    "print(len(lisdir))\n",
    "\n",
    "files = pd.DataFrame({'file_name':lisdir})\n",
    "files['StudyInstanceUID'] = files['file_name'].str.replace('.npy','')\n",
    "files = pd.merge(files, test)\n",
    "files = files.drop(columns=['SeriesInstanceUID', 'SOPInstanceUID','ycoord'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17b7f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 64\n",
    "MAX_SEQ_LENGTH = 200\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    \"\"\"Generates data for Keras\n",
    "    Sequence based data generator. Suitable for building data generator for training and prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, list_IDs, study_ids, num_features, directory,\n",
    "                 seq_length, return_type = 'both', to_fit=True, batch_size=32, \n",
    "                 shuffle=True, full_set = None, random=False, set_type='test',\n",
    "                 feat_type = 'feats', oversample=True, crop=False ):\n",
    "\n",
    "        self.list_IDs = list_IDs\n",
    "        self.study_ids = study_ids\n",
    "        self.to_fit = to_fit\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_features = num_features\n",
    "        self.seq_length = seq_length\n",
    "        self.full_set = full_set\n",
    "        self.return_type = return_type\n",
    "        self.random = random\n",
    "        warnings.filterwarnings(action='ignore')\n",
    "        self.directory = directory\n",
    "        self.set_type = set_type\n",
    "        self.feat_type = feat_type\n",
    "        self.oversample=oversample\n",
    "        self.crop= crop\n",
    "        \n",
    "        if self.set_type == 'val':\n",
    "            sample_pos = self.full_set[self.full_set.negative_exam_for_pe == False]\n",
    "            sample_neg = self.full_set[self.full_set.negative_exam_for_pe == True]\n",
    "            self.study_ids = pd.concat([sample_pos, sample_neg.sample(n=len(sample_pos))]).sample(frac = 1).reset_index(drop=True)\n",
    "            self.list_IDs = np.arange(0, len(self.study_ids))\n",
    "        self.on_epoch_end()\n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\n",
    "        :return: number of batches per epoch\n",
    "        \"\"\"\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "#         print('starting')\n",
    "        indexes = self.list_IDs[index * self.batch_size:((index+1) * self.batch_size)]\n",
    "        x_feats = np.zeros([self.batch_size, self.seq_length, self.num_features])\n",
    "        y_seq = np.zeros([self.batch_size, self.seq_length, 1])\n",
    "        y_tot = np.zeros([self.batch_size, 1])\n",
    "        for i in range(0,self.batch_size):\n",
    "            x, y = self._get_scan_data(self.study_ids.iloc[indexes[i]].StudyInstanceUID)\n",
    "            x_feats[i] = x\n",
    "            y_seq[i] = y[0]\n",
    "            y_tot[i] = y[1]\n",
    "            \n",
    "        if self.to_fit:\n",
    "            if self.return_type =='both':\n",
    "                y = [np.array(y_seq), y_tot]\n",
    "            elif self.return_type == 'seq':\n",
    "                y = y_seq\n",
    "            elif self.return_type == 'tot':\n",
    "                y = y_tot\n",
    "            else:\n",
    "                print('valid return types are both, seq and tot')\n",
    "                return False\n",
    "            x = x_feats\n",
    "            return x, y\n",
    "        else:\n",
    "            return (X)\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\n",
    "        \"\"\"\n",
    "        if self.set_type == 'train':\n",
    "            sample_pos = self.full_set[self.full_set.negative_exam_for_pe == False]\n",
    "            sample_neg = self.full_set[self.full_set.negative_exam_for_pe == True]\n",
    "            if self.oversample == False:\n",
    "                self.study_ids = pd.concat([sample_pos, sample_neg.sample(n=len(sample_pos))]).sample(frac=1).reset_index(drop=True)\n",
    "            else:\n",
    "                self.study_ids = pd.concat([sample_pos.sample(n=len(sample_neg), replace=True), sample_neg]).sample(frac = 1).reset_index(drop=True)\n",
    "            self.list_IDs = np.arange(0, len(self.study_ids))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.list_IDs)\n",
    "            \n",
    "    def _get_scan_data(self, study_id):\n",
    "        scan = pd.DataFrame(np.load(self.directory +study_id + '.npy', allow_pickle=True).tolist())\n",
    "        scan.ycoord = scan.ycoord.replace('True', '1.0').astype('float')\n",
    "        scan = scan.sort_values(by=['ycoord'], ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        features = scan.features.tolist()\n",
    "        seq = scan.pe_present_on_image.tolist()\n",
    "        seq = np.reshape(seq, [len(seq),1])\n",
    "        tot = self.study_ids[self.study_ids.StudyInstanceUID == study_id].negative_exam_for_pe.iloc[0]\n",
    "        \n",
    "        if self.set_type == 'train' and self.random == True:\n",
    "            new_len = int(len(scan) * (random.random()*0.25 +0.9))\n",
    "            inst = np.round(np.arange(0,new_len)/new_len*len(features)).astype(int)\n",
    "            features = (np.array(features)[inst]).tolist()\n",
    "            seq = seq[inst]   \n",
    "            \n",
    "            new_len = int(len(features)*(1 - np.random.random() * 0.05))\n",
    "            new_start = int(np.random.random() * (len(features)- new_len))\n",
    "            features = features[new_start:new_start+new_len]\n",
    "            seq = seq[new_start:new_start+new_len]        \n",
    "        \n",
    "        \n",
    "        if len(features)>=self.seq_length:\n",
    "            if self.crop==False:\n",
    "                inst = np.round(np.arange(0,self.seq_length)/self.seq_length*len(features)).astype(int)\n",
    "                xs = (np.array(features)[inst]).tolist()\n",
    "                ys = seq[inst]\n",
    "            if self.crop==True:\n",
    "                xs = (np.array(features)[:self.seq_length]).tolist()\n",
    "                ys = seq[:self.seq_length]\n",
    "        else:\n",
    "            xs = np.zeros([self.seq_length, self.num_features])\n",
    "            ys = np.zeros([self.seq_length, 1])\n",
    "\n",
    "            start = int(np.floor((self.seq_length - len(features))/2))\n",
    "            end = start + len(features)\n",
    "            xs[start:end] = features\n",
    "            ys[start:end] = seq\n",
    "\n",
    "        ys = ys.tolist()\n",
    "                    \n",
    "        return (xs, [ys, tot])\n",
    "\n",
    "    \n",
    "def get_generators(df, fold, batch_size, out, feat, random, seq_len=200, crop=True, np_dir='/home/shared/coat_np_0', osa=True, test_only=False):\n",
    "\n",
    "    dire = np_dir + str(fold) + '/'\n",
    "\n",
    "    if test_only:\n",
    "        return DataGenerator(np.arange(0, len(df)),\n",
    "                                    df, \n",
    "                                    64,\n",
    "                                    dire,\n",
    "                                    seq_len,\n",
    "                                    batch_size=batch_size,\n",
    "                                    set_type = 'test',\n",
    "                                    return_type=out,\n",
    "                                    feat_type=feat,\n",
    "                                    crop=crop,\n",
    "                                    shuffle=False)\n",
    "    \n",
    "    test_df = df[df.fold == fold]\n",
    "    train_df = df[df.fold != fold]\n",
    "        \n",
    "    print(train_df.fold.unique())\n",
    "    print(test_df.fold.unique())\n",
    "    test_generator = DataGenerator(np.arange(0, len(test_df)),\n",
    "                                    test_df, \n",
    "                                    64,\n",
    "                                    dire,\n",
    "                                    seq_len,\n",
    "                                    batch_size=batch_size,\n",
    "                                    set_type = 'test',\n",
    "                                    return_type=out,\n",
    "                                    feat_type=feat,\n",
    "                                    crop=crop)\n",
    "    \n",
    "    val_generator = DataGenerator(np.arange(0, len(test_df)),\n",
    "                                  test_df, \n",
    "                                  64,\n",
    "                                  dire,\n",
    "                                  seq_len,\n",
    "                                  batch_size=batch_size,\n",
    "                                  set_type ='val',\n",
    "                                  full_set=test_df,\n",
    "                                  return_type=out,\n",
    "                                  feat_type=feat,\n",
    "                                  crop=crop)\n",
    "    \n",
    "    train_generator = DataGenerator(np.arange(0, len(train_df)),\n",
    "                                  train_df, \n",
    "                                  64,\n",
    "                                  dire,\n",
    "                                  seq_len,\n",
    "                                  batch_size=batch_size,\n",
    "                                  set_type ='train',\n",
    "                                  full_set = train_df,\n",
    "                                  random = random,\n",
    "                                  return_type=out,\n",
    "                                  feat_type=feat,\n",
    "                                  crop=crop,\n",
    "                                  oversample=osa)\n",
    "    \n",
    "    return train_generator, val_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c54eeaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_attn_lstm(seq_len, noise=0.001):\n",
    "        num_heads = 1\n",
    "        key_dim = 8\n",
    "        inputs = keras.Input(shape=(seq_len, 64))\n",
    "        x = layers.BatchNormalization()(inputs)\n",
    "        x = layers.GaussianNoise(noise)(x)\n",
    "        x = layers.TimeDistributed(layers.Dropout(0.3))(x)\n",
    "        attn = layers.MultiHeadAttention(num_heads=num_heads,\n",
    "                                                     key_dim=key_dim, \n",
    "                                                     dropout=0.4\n",
    "                                                    )(x, x)\n",
    "        attn = layers.LayerNormalization()(attn)\n",
    "        x = layers.Add()((x,attn))\n",
    "        x = layers.TimeDistributed(layers.Dropout(0.4))(x)\n",
    "        x = layers.Bidirectional(layers.LSTM(units=8, \n",
    "                                             return_sequences=True,\n",
    "                                             dropout=0.3, \n",
    "                                             recurrent_regularizer=keras.regularizers.L2(0.01),\n",
    "                                            ))(x)\n",
    "        x = layers.TimeDistributed(layers.Dropout(0.4))(x)\n",
    "        x = layers.LSTM(units=8, return_sequences=True,\n",
    "                        dropout=0.3, \n",
    "#                         bias_regularizer=keras.regularizers.L2(0.005)\n",
    "                       )(x)\n",
    "        x = layers.TimeDistributed(layers.Dropout(0.4))(x)\n",
    "        x = layers.Bidirectional(layers.LSTM(units=8,\n",
    "                                             return_sequences=False, \n",
    "                                             dropout=0.3,\n",
    "#                                              bias_regularizer=keras.regularizers.L2(0.005),\n",
    "                                            ))(x)\n",
    "        x = layers.Dropout(0.4)(x)\n",
    "        stack_outputs = layers.Dense(1, activation='sigmoid', name='tot_out')(x)\n",
    "\n",
    "        model = keras.models.Model(inputs=inputs, outputs=stack_outputs)\n",
    "        \n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "536ed5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TurkuGenerator(keras.utils.Sequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "#         encoding_dict: Dict[str, np.ndarray],\n",
    "#         filename_col: str,\n",
    "#         label_col: str,\n",
    "#         dataset_label_col: str,\n",
    "        batch_size: int,\n",
    "        encoding_dim: int,\n",
    "        directory: str,\n",
    "        num_slices: int = 96,\n",
    "        shuffle: bool = True,\n",
    "    ):\n",
    "        self.df = df.copy()\n",
    "        self.series_instance_uids = self.df[\"StudyInstanceUID\"].unique()\n",
    "        self.count_no_aug_series = len(self.series_instance_uids)\n",
    "#         self.encoding_dict = encoding_dict\n",
    "#         self.filename_col = filename_col\n",
    "#         self.label_col = label_col\n",
    "#         self.dataset_label_col = dataset_label_col\n",
    "        self.batch_size = batch_size\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.num_slices = num_slices\n",
    "        self.shuffle = shuffle\n",
    "        self.directory = directory\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return int(np.ceil(self.count_no_aug_series / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        offset = index * self.batch_size\n",
    "        indexes = self.indexes[offset : min(offset + self.batch_size, self.df.shape[0])]\n",
    "\n",
    "        UIDs = [self.series_instance_uids[k] for k in indexes]\n",
    "\n",
    "        X, ys = self._prepare_batch(UIDs)\n",
    "\n",
    "        return X, ys\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\"\"\"\n",
    "        self.indexes = np.arange(len(self.series_instance_uids))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def _prepare_batch(self, UIDs):\n",
    "        batch_size = len(UIDs)\n",
    "        X = np.zeros((batch_size, self.num_slices, self.encoding_dim), dtype=np.float32)\n",
    "        y = np.zeros((batch_size, self.num_slices, 1), dtype=np.int32)\n",
    "        y_stack = np.zeros((batch_size, 1), dtype=np.int32)\n",
    "\n",
    "        for i, uid in enumerate(UIDs):\n",
    "\n",
    "            scan = pd.DataFrame(np.load(self.directory + uid + '.npy', allow_pickle=True).tolist())\n",
    "            scan.ycoord = scan.ycoord.replace('True', '1.0').astype('float')\n",
    "            scan = scan.sort_values(by=['ycoord'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "            features = scan.features.tolist()\n",
    "            seq = scan.pe_present_on_image.tolist()\n",
    "            \n",
    "            if len(features) > self.num_slices:\n",
    "                X[i] = features[:self.num_slices]\n",
    "                y[i,:,0] = seq[:self.num_slices]\n",
    "            else:\n",
    "                X[i, :len(features), :] = features\n",
    "                y[i, :len(seq), 0] = seq\n",
    "#             y_stack[i,0] = scan.negative_exam_for_pe.iloc[0]\n",
    "            y_stack[i,0] = 0 \n",
    "            \n",
    "        return X, [y, y_stack]\n",
    "\n",
    "\n",
    "def get_turku_generators(df, fold, batch_size, np_dir='/home/shared/nps/turku_0', test_only=False):\n",
    "    directory = np_dir + str(fold) + '/'\n",
    "\n",
    "    if test_only == True:\n",
    "        test_generator = TurkuGenerator(df, \n",
    "                                       batch_size,\n",
    "                                       64,\n",
    "                                       directory,\n",
    "                                       shuffle=False)\n",
    "        return test_generator\n",
    "    \n",
    "    test_df = df[df.fold == fold]\n",
    "    train_df = df[df.fold != fold]\n",
    "    \n",
    "    train_pos = train_df[train_df.negative_exam_for_pe == False]\n",
    "    train_neg = train_df[train_df.negative_exam_for_pe == True]\n",
    "\n",
    "    val_pos = test_df[test_df.negative_exam_for_pe == False]\n",
    "    val_neg = test_df[test_df.negative_exam_for_pe == True]\n",
    "\n",
    "    train = pd.concat([train_pos, train_neg.sample(n=len(train_pos))]).sample(frac = 1).reset_index(drop=True)\n",
    "    val = pd.concat([val_pos, val_neg.sample(n=len(val_pos))]).sample(frac = 1).reset_index(drop=True)\n",
    "    \n",
    "#     print(train_df.fold.unique())\n",
    "#     print(test_df.fold.unique())\n",
    "\n",
    "    test_generator = TurkuGenerator(test_df,\n",
    "                                    batch_size,\n",
    "                                    64,\n",
    "                                    directory)\n",
    "    \n",
    "    val_generator = TurkuGenerator(val,\n",
    "                                    batch_size,\n",
    "                                    64,\n",
    "                                    directory)\n",
    "    \n",
    "    train_generator = TurkuGenerator(train,\n",
    "                                    batch_size,\n",
    "                                    64,\n",
    "                                    directory)\n",
    "    \n",
    "    return train_generator, val_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ac2d7241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tukru_model() -> keras.models.Model:\n",
    "    \"\"\"Get a custom LSTM based model for classifying a set of slice encodings.\n",
    "    The model predicts presence of PE at both slice and stack level.\n",
    "    \"\"\"\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    slice_stack = keras.Input(shape=(NUM_SLICES, ENCODING_DIM))\n",
    "\n",
    "    x = layers.BatchNormalization()(slice_stack)\n",
    "    x = layers.Bidirectional(layers.LSTM(units=64, return_sequences=True))(x)\n",
    "    x = layers.TimeDistributed(layers.Dense(512, activation=\"relu\"))(x)\n",
    "    x = layers.TimeDistributed(layers.Dense(128, activation=\"relu\"))(x)\n",
    "    slice_outputs = layers.TimeDistributed(layers.Dense(1, activation=\"sigmoid\"), name=\"slices\")(x)\n",
    "\n",
    "    reshaped_features = layers.Reshape((NUM_SLICES,))(slice_outputs)\n",
    "    reshaped_features = layers.Dropout(0.05)(reshaped_features)\n",
    "    stack_output = layers.Dense(1, activation=\"sigmoid\", name=\"sequence\")(reshaped_features)\n",
    "\n",
    "    model = keras.models.Model(inputs=slice_stack, outputs=[slice_outputs, stack_output])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbafbbfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq_len=208\n",
    "tot = 0\n",
    "caught = 0\n",
    "\n",
    "no = 0\n",
    "nope = 0\n",
    "\n",
    "test = files.sort_values('StudyInstanceUID')\n",
    "test['negative_exam_for_pe'] = 0\n",
    "for fold in range(0,1):\n",
    "\n",
    "    model = get_attn_lstm(seq_len)\n",
    "\n",
    "    np_dir = '/home/shared/test/coat_0'\n",
    "\n",
    "    batch_size = 1\n",
    "    test_gen = get_generators(test, fold, \n",
    "                                   batch_size, \n",
    "                                   'tot', \n",
    "                                   'feats', \n",
    "                                   True, \n",
    "                                   seq_len=seq_len, \n",
    "                                   np_dir=np_dir, \n",
    "                                   crop=False,\n",
    "                                   osa=False,\n",
    "                                   test_only=True)\n",
    "    \n",
    "    t_len = test_gen.__len__()\n",
    "    x_test = np.zeros([t_len,208,num_features])\n",
    "    y_test = np.zeros([t_len,1])\n",
    "    for i in range(0,t_len):\n",
    "        x_test[i],y= test_gen.__getitem__(i)\n",
    "        y_test[i] = y\n",
    "                \n",
    "    lr = 0.001\n",
    "    opt = keras.optimizers.Nadam(learning_rate=lr)\n",
    "    model.compile(\n",
    "            optimizer=opt,\n",
    "            loss=keras.losses.binary_crossentropy,\n",
    "            metrics=[\"accuracy\",]\n",
    "        )\n",
    "\n",
    "    checkpoint_filepath = '/home/shared/model_checkpoint_paige/scan/reg5-kfold_0'+str(fold)\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    \n",
    "    y_pred = model.predict(x_test, batch_size=32)\n",
    "    test['fold_0'+str(fold)] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e51b8ddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = files.sort_values('StudyInstanceUID')\n",
    "test['negative_exam_for_pe'] = 0\n",
    "\n",
    "NUM_SLICES=96\n",
    "ENCODING_DIM = 64\n",
    "for fold in range(0,10):\n",
    "\n",
    "    model = get_tukru_model()\n",
    "    opt = keras.optimizers.Nadam(learning_rate=0.001)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "    np_dir = '/home/shared/test/turku_0'\n",
    "\n",
    "    batch_size = 1\n",
    "    test_gen = get_turku_generators(test,fold,1,np_dir=np_dir,test_only=True)    \n",
    "    \n",
    "    t_len = test_gen.__len__()\n",
    "    x_test = np.zeros([t_len,NUM_SLICES,ENCODING_DIM])\n",
    "    y_test = np.zeros([t_len,1])\n",
    "    for i in range(0,t_len):\n",
    "        x_test[i],y= test_gen.__getitem__(i)\n",
    "        y_test[i] = y[1]\n",
    "                \n",
    "\n",
    "    checkpoint_filepath = '/home/shared/model_checkpoint_paige/scan/turku_fold_0'+str(fold)\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    \n",
    "    y_pred = model.predict(x_test, batch_size=32)\n",
    "    test['fold_0'+str(fold)] = y_pred[1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8b84d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans['avg'] = 0\n",
    "for fold in range(0,10): \n",
    "    ans['avg'] += ans['fold_0'+str(fold)]/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "1f7ff13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(0,10):\n",
    "    ans['fold_0'+str(fold)] = 1-ans['fold_0'+str(fold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7159d4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>StudyInstanceUID</th>\n",
       "      <th>fold_00</th>\n",
       "      <th>fold_01</th>\n",
       "      <th>fold_02</th>\n",
       "      <th>fold_03</th>\n",
       "      <th>fold_04</th>\n",
       "      <th>fold_05</th>\n",
       "      <th>fold_06</th>\n",
       "      <th>fold_07</th>\n",
       "      <th>fold_08</th>\n",
       "      <th>fold_09</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>362</td>\n",
       "      <td>00268ff88746</td>\n",
       "      <td>0.516448</td>\n",
       "      <td>0.101103</td>\n",
       "      <td>0.485995</td>\n",
       "      <td>0.091825</td>\n",
       "      <td>0.291079</td>\n",
       "      <td>0.127315</td>\n",
       "      <td>0.232077</td>\n",
       "      <td>0.410579</td>\n",
       "      <td>0.382904</td>\n",
       "      <td>0.142558</td>\n",
       "      <td>0.278188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>313</td>\n",
       "      <td>00c53115a9fa</td>\n",
       "      <td>0.344861</td>\n",
       "      <td>0.342433</td>\n",
       "      <td>0.693427</td>\n",
       "      <td>0.466849</td>\n",
       "      <td>0.327386</td>\n",
       "      <td>0.290157</td>\n",
       "      <td>0.232099</td>\n",
       "      <td>0.068455</td>\n",
       "      <td>0.140639</td>\n",
       "      <td>0.076027</td>\n",
       "      <td>0.298233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>514</td>\n",
       "      <td>00e7015490cb</td>\n",
       "      <td>0.193868</td>\n",
       "      <td>0.071207</td>\n",
       "      <td>0.059365</td>\n",
       "      <td>0.076562</td>\n",
       "      <td>0.030576</td>\n",
       "      <td>0.073124</td>\n",
       "      <td>0.055059</td>\n",
       "      <td>0.068053</td>\n",
       "      <td>0.103279</td>\n",
       "      <td>0.112114</td>\n",
       "      <td>0.084321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>017d4bd72a02</td>\n",
       "      <td>0.983563</td>\n",
       "      <td>0.987083</td>\n",
       "      <td>0.991697</td>\n",
       "      <td>0.916703</td>\n",
       "      <td>0.994702</td>\n",
       "      <td>0.994667</td>\n",
       "      <td>0.992388</td>\n",
       "      <td>0.992160</td>\n",
       "      <td>0.990097</td>\n",
       "      <td>0.994776</td>\n",
       "      <td>0.983783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>0185602e0766</td>\n",
       "      <td>0.082452</td>\n",
       "      <td>0.097081</td>\n",
       "      <td>0.055920</td>\n",
       "      <td>0.077840</td>\n",
       "      <td>0.033091</td>\n",
       "      <td>0.124356</td>\n",
       "      <td>0.054448</td>\n",
       "      <td>0.057666</td>\n",
       "      <td>0.064574</td>\n",
       "      <td>0.056883</td>\n",
       "      <td>0.070431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>27</td>\n",
       "      <td>fda6fb996297</td>\n",
       "      <td>0.044739</td>\n",
       "      <td>0.072529</td>\n",
       "      <td>0.093087</td>\n",
       "      <td>0.075421</td>\n",
       "      <td>0.030814</td>\n",
       "      <td>0.023850</td>\n",
       "      <td>0.062034</td>\n",
       "      <td>0.058166</td>\n",
       "      <td>0.078919</td>\n",
       "      <td>0.059445</td>\n",
       "      <td>0.059900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>476</td>\n",
       "      <td>fdbbd076dcdb</td>\n",
       "      <td>0.184319</td>\n",
       "      <td>0.098847</td>\n",
       "      <td>0.418313</td>\n",
       "      <td>0.206726</td>\n",
       "      <td>0.237240</td>\n",
       "      <td>0.362425</td>\n",
       "      <td>0.111578</td>\n",
       "      <td>0.270715</td>\n",
       "      <td>0.154030</td>\n",
       "      <td>0.159400</td>\n",
       "      <td>0.220359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>336</td>\n",
       "      <td>fde9ecc1c2b5</td>\n",
       "      <td>0.080061</td>\n",
       "      <td>0.118764</td>\n",
       "      <td>0.094568</td>\n",
       "      <td>0.076470</td>\n",
       "      <td>0.034160</td>\n",
       "      <td>0.024418</td>\n",
       "      <td>0.090630</td>\n",
       "      <td>0.062398</td>\n",
       "      <td>0.109138</td>\n",
       "      <td>0.059092</td>\n",
       "      <td>0.074970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>517</td>\n",
       "      <td>fe449a869d7a</td>\n",
       "      <td>0.290019</td>\n",
       "      <td>0.119091</td>\n",
       "      <td>0.094762</td>\n",
       "      <td>0.075550</td>\n",
       "      <td>0.172891</td>\n",
       "      <td>0.028744</td>\n",
       "      <td>0.057242</td>\n",
       "      <td>0.058463</td>\n",
       "      <td>0.120106</td>\n",
       "      <td>0.091727</td>\n",
       "      <td>0.110860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>481</td>\n",
       "      <td>ff62ec60c99b</td>\n",
       "      <td>0.937609</td>\n",
       "      <td>0.985631</td>\n",
       "      <td>0.951850</td>\n",
       "      <td>0.821194</td>\n",
       "      <td>0.853823</td>\n",
       "      <td>0.994683</td>\n",
       "      <td>0.992123</td>\n",
       "      <td>0.990539</td>\n",
       "      <td>0.989644</td>\n",
       "      <td>0.979381</td>\n",
       "      <td>0.949648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>650 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0 StudyInstanceUID   fold_00   fold_01   fold_02   fold_03  \\\n",
       "0           362     00268ff88746  0.516448  0.101103  0.485995  0.091825   \n",
       "1           313     00c53115a9fa  0.344861  0.342433  0.693427  0.466849   \n",
       "2           514     00e7015490cb  0.193868  0.071207  0.059365  0.076562   \n",
       "3             8     017d4bd72a02  0.983563  0.987083  0.991697  0.916703   \n",
       "4            41     0185602e0766  0.082452  0.097081  0.055920  0.077840   \n",
       "..          ...              ...       ...       ...       ...       ...   \n",
       "645          27     fda6fb996297  0.044739  0.072529  0.093087  0.075421   \n",
       "646         476     fdbbd076dcdb  0.184319  0.098847  0.418313  0.206726   \n",
       "647         336     fde9ecc1c2b5  0.080061  0.118764  0.094568  0.076470   \n",
       "648         517     fe449a869d7a  0.290019  0.119091  0.094762  0.075550   \n",
       "649         481     ff62ec60c99b  0.937609  0.985631  0.951850  0.821194   \n",
       "\n",
       "      fold_04   fold_05   fold_06   fold_07   fold_08   fold_09       avg  \n",
       "0    0.291079  0.127315  0.232077  0.410579  0.382904  0.142558  0.278188  \n",
       "1    0.327386  0.290157  0.232099  0.068455  0.140639  0.076027  0.298233  \n",
       "2    0.030576  0.073124  0.055059  0.068053  0.103279  0.112114  0.084321  \n",
       "3    0.994702  0.994667  0.992388  0.992160  0.990097  0.994776  0.983783  \n",
       "4    0.033091  0.124356  0.054448  0.057666  0.064574  0.056883  0.070431  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "645  0.030814  0.023850  0.062034  0.058166  0.078919  0.059445  0.059900  \n",
       "646  0.237240  0.362425  0.111578  0.270715  0.154030  0.159400  0.220359  \n",
       "647  0.034160  0.024418  0.090630  0.062398  0.109138  0.059092  0.074970  \n",
       "648  0.172891  0.028744  0.057242  0.058463  0.120106  0.091727  0.110860  \n",
       "649  0.853823  0.994683  0.992123  0.990539  0.989644  0.979381  0.949648  \n",
       "\n",
       "[650 rows x 13 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f3e02a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.0"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.abs(test[:200].fold_00.round() - test[:200].fold_06.round())).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "88d9f399",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = ans.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "485c0b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans.to_csv('test_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "27c81f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_ans['StudyInstanceUID'] = ip_ans.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5d1671fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = pd.merge(ip_ans, test, on='StudyInstanceUID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d8b6fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_ans['StudyInstanceUID'] = ip_ans.id.str.replace('_negative_exam_for_pe','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "45c577c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = pd.merge(ip_ans, test, on='StudyInstanceUID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f9bc9b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = comp.drop(columns=['StudyInstanceUID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c4f18e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp['sum'] = 0\n",
    "for fold in range(0,10): \n",
    "    comp['sum'] = comp['sum'] + comp['fold_0' +str(fold)].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b39a3ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_preds.csv')\n",
    "bench = pd.read_csv('benchmark_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0545323e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3738914155132462"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.avg.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "41109262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5037020651384616"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bench.fold_00.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "2a16d27a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209.0, 194.0)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(bench.fold_00.round()), np.sum(test.fold_00.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "c3746d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09846153846153846"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(test.avg.round() - test.fold_00.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "b56351dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(columns=['Accuracy','Sensitivity','Specificity','PPV',\n",
    "                             'NPV','F1Score','ROC_AUC'])\n",
    "\n",
    "test = test.append([{'Accuracy': 89.23,\n",
    "    'Sensitivity': 82.29,\n",
    "    'Specificity': 92.14,\n",
    "    'PPV': 81.44,\n",
    "    'NPV': 92.54,\n",
    "    'F1Score': 0.8186528497409326,\n",
    "    'ROC_AUC': 0.9212609170305677},\n",
    "    {'Accuracy': 86.46,\n",
    "    'Sensitivity': 82.81,\n",
    "    'Specificity': 87.99,\n",
    "    'PPV': 74.3,\n",
    "    'NPV': 92.43,\n",
    "    'F1Score': 0.7832512315270936,\n",
    "    'ROC_AUC': 0.9276519286754004},\n",
    "    {'Accuracy': 87.69,\n",
    "    'Sensitivity': 83.85,\n",
    "    'Specificity': 89.3,\n",
    "    'PPV': 76.67,\n",
    "    'NPV': 92.95,\n",
    "    'F1Score': 0.8009950248756219,\n",
    "    'ROC_AUC': 0.9254798944687045},\n",
    "    {'Accuracy': 87.23,\n",
    "    'Sensitivity': 78.65,\n",
    "    'Specificity': 90.83,\n",
    "    'PPV': 78.24,\n",
    "    'NPV': 91.03,\n",
    "    'F1Score': 0.7844155844155843,\n",
    "    'ROC_AUC': 0.9140170123726347},\n",
    "    {'Accuracy': 87.38,\n",
    "    'Sensitivity': 79.69,\n",
    "    'Specificity': 90.61,\n",
    "    'PPV': 78.06,\n",
    "    'NPV': 91.41,\n",
    "    'F1Score': 0.788659793814433,\n",
    "    'ROC_AUC': 0.9151314592430859},\n",
    "    {'Accuracy': 88.92,\n",
    "    'Sensitivity': 81.25,\n",
    "    'Specificity': 92.14,\n",
    "    'PPV': 81.25,\n",
    "    'NPV': 92.14,\n",
    "    'F1Score': 0.8125,\n",
    "    'ROC_AUC': 0.9324053857350799,},\n",
    "    {'Accuracy': 87.08,\n",
    "    'Sensitivity': 81.25,\n",
    "    'Specificity': 89.52,\n",
    "    'PPV': 76.47,\n",
    "    'NPV': 91.93,\n",
    "    'F1Score': 0.787878787878788,\n",
    "    'ROC_AUC': 0.9285161935953421},\n",
    "    {'Accuracy': 86.92,\n",
    "    'Sensitivity': 77.6,\n",
    "    'Specificity': 90.83,\n",
    "    'PPV': 78.01,\n",
    "    'NPV': 90.63,\n",
    "    'F1Score': 0.7780678851174934,\n",
    "    'ROC_AUC': 0.9140738719068414},\n",
    "    {'Accuracy': 87.85,\n",
    "    'Sensitivity': 81.77,\n",
    "    'Specificity': 90.39,\n",
    "    'PPV': 78.11,\n",
    "    'NPV': 92.2,\n",
    "    'F1Score': 0.7989821882951654,\n",
    "    'ROC_AUC': 0.9159502365356623},\n",
    "    {'Accuracy': 87.69,\n",
    "    'Sensitivity': 79.69,\n",
    "    'Specificity': 91.05,\n",
    "    'PPV': 78.87,\n",
    "    'NPV': 91.45,\n",
    "    'F1Score': 0.7927461139896373,\n",
    "    'ROC_AUC': 0.9081377365356624}])\n",
    "\n",
    "# 'Accuracy': 89.69,\n",
    "# 'Sensitivity': 80.73,\n",
    "# 'Specificity': 93.45,\n",
    "# 'PPV': 83.78,\n",
    "# 'NPV': 92.04,\n",
    "# 'F1Score': 0.8222811671087533,\n",
    "# 'ROC_AUC': 0.9374545123726347\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "1ec73da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = pd.DataFrame(columns=['Accuracy','Sensitivity','Specificity','PPV',\n",
    "                             'NPV','F1Score','ROC_AUC'])\n",
    "\n",
    "bench = bench.append([{'Accuracy': 84.15,\n",
    "    'Sensitivity': 77.6,\n",
    "    'Specificity': 86.9,\n",
    "    'PPV': 71.29,\n",
    "    'NPV': 90.25,\n",
    "    'F1Score': 0.743142144638404,\n",
    "    'ROC_AUC': 0.8819141193595342},\n",
    "    {'Accuracy': 86.31,\n",
    "    'Sensitivity': 71.88,\n",
    "    'Specificity': 92.36,\n",
    "    'PPV': 79.77,\n",
    "    'NPV': 88.68,\n",
    "    'F1Score': 0.7561643835616437,\n",
    "    'ROC_AUC': 0.8922056950509462},\n",
    "    {'Accuracy': 84.77,\n",
    "    'Sensitivity': 76.04,\n",
    "    'Specificity': 88.43,\n",
    "    'PPV': 73.37,\n",
    "    'NPV': 89.8,\n",
    "    'F1Score': 0.7468030690537084,\n",
    "    'ROC_AUC': 0.8944232168850071},\n",
    "    {'Accuracy': 87.69,\n",
    "    'Sensitivity': 76.56,\n",
    "    'Specificity': 92.36,\n",
    "    'PPV': 80.77,\n",
    "    'NPV': 90.38,\n",
    "    'F1Score': 0.786096256684492,\n",
    "    'ROC_AUC': 0.8911822234352257},\n",
    "    {'Accuracy': 87.85,\n",
    "    'Sensitivity': 74.48,\n",
    "    'Specificity': 93.45,\n",
    "    'PPV': 82.66,\n",
    "    'NPV': 89.73,\n",
    "    'F1Score': 0.7835616438356164,\n",
    "    'ROC_AUC': 0.9017012372634643},\n",
    "    {'Accuracy': 84.46,\n",
    "    'Sensitivity': 77.08,\n",
    "    'Specificity': 87.55,\n",
    "    'PPV': 72.2,\n",
    "    'NPV': 90.11,\n",
    "    'F1Score': 0.7455919395465995,\n",
    "    'ROC_AUC': 0.9017467248908295},\n",
    "    {'Accuracy': 85.54,\n",
    "    'Sensitivity': 68.23,\n",
    "    'Specificity': 92.79,\n",
    "    'PPV': 79.88,\n",
    "    'NPV': 87.45,\n",
    "    'F1Score': 0.7359550561797754,\n",
    "    'ROC_AUC': 0.8890443049490538},\n",
    "    {'Accuracy': 85.23,\n",
    "    'Sensitivity': 78.65,\n",
    "    'Specificity': 87.99,\n",
    "    'PPV': 73.3,\n",
    "    'NPV': 90.77,\n",
    "    'F1Score': 0.7587939698492463,\n",
    "    'ROC_AUC': 0.9007118813682679},\n",
    "    {'Accuracy': 85.54,\n",
    "    'Sensitivity': 72.4,\n",
    "    'Specificity': 91.05,\n",
    "    'PPV': 77.22,\n",
    "    'NPV': 88.72,\n",
    "    'F1Score': 0.7473118279569892,\n",
    "    'ROC_AUC': 0.8927970342066959},\n",
    "    {'Accuracy': 85.08,\n",
    "    'Sensitivity': 74.48,\n",
    "    'Specificity': 89.52,\n",
    "    'PPV': 74.87,\n",
    "    'NPV': 89.32,\n",
    "    'F1Score': 0.7467362924281983,\n",
    "    'ROC_AUC': 0.8900791484716157}])\n",
    "# Evaluation of avg\n",
    "# 'Accuracy': 86.46,\n",
    "# 'Sensitivity': 76.56,\n",
    "# 'Specificity': 90.61,\n",
    "# 'PPV': 77.37,\n",
    "# 'NPV': 90.22,\n",
    "# 'F1Score': 0.769633507853403\n",
    "# 'ROC_AUC': 0.905510826055313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "e658f493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>PPV</th>\n",
       "      <th>NPV</th>\n",
       "      <th>F1Score</th>\n",
       "      <th>ROC_AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.23</td>\n",
       "      <td>82.29</td>\n",
       "      <td>92.14</td>\n",
       "      <td>81.44</td>\n",
       "      <td>92.54</td>\n",
       "      <td>0.818653</td>\n",
       "      <td>0.921261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86.46</td>\n",
       "      <td>82.81</td>\n",
       "      <td>87.99</td>\n",
       "      <td>74.30</td>\n",
       "      <td>92.43</td>\n",
       "      <td>0.783251</td>\n",
       "      <td>0.927652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.69</td>\n",
       "      <td>83.85</td>\n",
       "      <td>89.30</td>\n",
       "      <td>76.67</td>\n",
       "      <td>92.95</td>\n",
       "      <td>0.800995</td>\n",
       "      <td>0.925480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>87.23</td>\n",
       "      <td>78.65</td>\n",
       "      <td>90.83</td>\n",
       "      <td>78.24</td>\n",
       "      <td>91.03</td>\n",
       "      <td>0.784416</td>\n",
       "      <td>0.914017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.38</td>\n",
       "      <td>79.69</td>\n",
       "      <td>90.61</td>\n",
       "      <td>78.06</td>\n",
       "      <td>91.41</td>\n",
       "      <td>0.788660</td>\n",
       "      <td>0.915131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>88.92</td>\n",
       "      <td>81.25</td>\n",
       "      <td>92.14</td>\n",
       "      <td>81.25</td>\n",
       "      <td>92.14</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.932405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>87.08</td>\n",
       "      <td>81.25</td>\n",
       "      <td>89.52</td>\n",
       "      <td>76.47</td>\n",
       "      <td>91.93</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.928516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>86.92</td>\n",
       "      <td>77.60</td>\n",
       "      <td>90.83</td>\n",
       "      <td>78.01</td>\n",
       "      <td>90.63</td>\n",
       "      <td>0.778068</td>\n",
       "      <td>0.914074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>87.85</td>\n",
       "      <td>81.77</td>\n",
       "      <td>90.39</td>\n",
       "      <td>78.11</td>\n",
       "      <td>92.20</td>\n",
       "      <td>0.798982</td>\n",
       "      <td>0.915950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>87.69</td>\n",
       "      <td>79.69</td>\n",
       "      <td>91.05</td>\n",
       "      <td>78.87</td>\n",
       "      <td>91.45</td>\n",
       "      <td>0.792746</td>\n",
       "      <td>0.908138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Sensitivity  Specificity    PPV    NPV   F1Score   ROC_AUC\n",
       "0     89.23        82.29        92.14  81.44  92.54  0.818653  0.921261\n",
       "1     86.46        82.81        87.99  74.30  92.43  0.783251  0.927652\n",
       "2     87.69        83.85        89.30  76.67  92.95  0.800995  0.925480\n",
       "3     87.23        78.65        90.83  78.24  91.03  0.784416  0.914017\n",
       "4     87.38        79.69        90.61  78.06  91.41  0.788660  0.915131\n",
       "5     88.92        81.25        92.14  81.25  92.14  0.812500  0.932405\n",
       "6     87.08        81.25        89.52  76.47  91.93  0.787879  0.928516\n",
       "7     86.92        77.60        90.83  78.01  90.63  0.778068  0.914074\n",
       "8     87.85        81.77        90.39  78.11  92.20  0.798982  0.915950\n",
       "9     87.69        79.69        91.05  78.87  91.45  0.792746  0.908138"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "d014148a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "mean test: 87.645 mean bench: 85.662 p-value 0.31731051\n",
      "RanksumsResult(statistic=2.7591406529673588, pvalue=0.00579535854433471)\n",
      "\n",
      "Sensitivity\n",
      "mean test: 80.885 mean bench: 74.740 p-value 0.31731051\n",
      "RanksumsResult(statistic=3.6284589408885815, pvalue=0.00028511808363161265)\n",
      "\n",
      "Specificity\n",
      "mean test: 90.480 mean bench: 90.240 p-value 0.31731051\n",
      "RanksumsResult(statistic=-0.11338934190276817, pvalue=0.9097218891455553)\n",
      "\n",
      "PPV\n",
      "mean test: 78.142 mean bench: 76.533 p-value 0.31731051\n",
      "RanksumsResult(statistic=0.9071147352221454, pvalue=0.3643461266335529)\n",
      "\n",
      "NPV\n",
      "mean test: 91.871 mean bench: 89.521 p-value 0.31731051\n",
      "RanksumsResult(statistic=3.704051835490427, pvalue=0.00021218287122257823)\n",
      "\n",
      "F1Score\n",
      "mean test: 0.795 mean bench: 0.755 p-value 0.31731051\n",
      "RanksumsResult(statistic=3.4016802570830453, pvalue=0.0006697294490218271)\n",
      "\n",
      "ROC_AUC\n",
      "mean test: 0.920 mean bench: 0.894 p-value 0.31731051\n",
      "RanksumsResult(statistic=3.779644730092272, pvalue=0.00015705228423075119)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ranksums\n",
    "for col in bench.columns:\n",
    "    print(col)\n",
    "    test[col] = test[col].astype('double')\n",
    "    bench[col] = bench[col].astype('double')\n",
    "    print('mean test: {:.3f} mean bench: {:.3f} p-value {:.8f}'.format(test[col].mean(), bench[col].mean(), ranksums(test[col][2], bench[col][2])[1]))\n",
    "    print(ranksums(test[col],bench[col]))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
