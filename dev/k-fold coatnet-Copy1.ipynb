{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "783291d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.1\n",
      "2.8.0\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce3d6b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random\n",
    "from keras import layers\n",
    "np.random.seed(1)\n",
    "random.seed(2)\n",
    "tf.random.set_seed(4)\n",
    "\n",
    "EPOCHS = 9\n",
    "MODEL_DIR = '/home/shared/model_checkpoint_paige/singlescan-3channel/'\n",
    "\n",
    "CROPPED = 0\n",
    "MASKED = 1\n",
    "ORIGINAL = 2\n",
    "\n",
    "IMG_TYPE = MASKED\n",
    "IMAGE_PATH = '/home/shared/nps/imgs/'\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "FOLDS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a477c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1771621/1656503385.py:1: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_ids = pd.read_csv('all_ids_updated.csv')\n"
     ]
    }
   ],
   "source": [
    "all_ids = pd.read_csv('all_ids_updated.csv')\n",
    "all_ids = all_ids[all_ids.contains_lung == True].sample(frac=1).reset_index(drop=True)\n",
    "all_ids.ycoord = all_ids.ycoord.replace('True', '1.0').astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f5a57cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_df_upd.csv').drop(columns='Unnamed: 0')\n",
    "test_df = pd.read_csv('test_df_upd.csv').drop(columns='Unnamed: 0')\n",
    "val_df = pd.read_csv('val_df_upd.csv').drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eb1a339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold_df = pd.DataFrame({'StudyInstanceUID': all_ids.StudyInstanceUID.unique()}).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72461bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold_df = fold_df.reset_index(drop=True)\n",
    "# fold_df['fold'] = fold_df.index % 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a6ed9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold_df.to_csv('folds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc805fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids.ycoord = all_ids.ycoord.astype('float')\n",
    "\n",
    "SLAB_SIZE = 3\n",
    "\n",
    "all_ids_slab = all_ids.sort_values(['StudyInstanceUID', 'ycoord'])\n",
    "all_ids_slab['endStudyUID'] = all_ids_slab.StudyInstanceUID.shift(SLAB_SIZE - 1)\n",
    "\n",
    "all_ids_slab['pe_in_slab'] = all_ids_slab.pe_present_on_image\n",
    "for i in range(1, SLAB_SIZE):\n",
    "    all_ids_slab['pe_' + str(i)] = all_ids_slab.pe_present_on_image.shift(i)\n",
    "    all_ids_slab['SOP_' + str(i)] = all_ids_slab.SOPInstanceUID.shift(i)\n",
    "    all_ids_slab['pe_in_slab'] = all_ids_slab['pe_in_slab'] + all_ids_slab['pe_' + str(i)]\n",
    "\n",
    "all_ids_slab['pe_in_slab'] = all_ids_slab.pe_1 == 1\n",
    "\n",
    "all_ids_slab = all_ids_slab[all_ids_slab.StudyInstanceUID == all_ids_slab.endStudyUID]\n",
    "\n",
    "all_ids_slab = all_ids_slab.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65726a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_df = pd.read_csv('folds.csv')\n",
    "all_ids_slab = pd.merge(all_ids_slab, fold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2cb1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model() -> keras.models.Model:\n",
    "    \n",
    "    from keras_cv_attention_models import coatnet\n",
    "    \n",
    "    inputs = keras.Input(shape=(256,256,3))\n",
    "    \n",
    "#     data_aug = keras.Sequential(\n",
    "#                     [\n",
    "#                         layers.RandomRotation(factor=0.05, fill_mode='constant'),\n",
    "#                         layers.RandomZoom(height_factor=0.1, width_factor=0.1, fill_mode ='constant'),\n",
    "#                         layers.RandomTranslation(0.1,0.1, fill_mode='constant')\n",
    "#                     ],\n",
    "#                     name=\"data_augmentation\",)\n",
    "    \n",
    "#     aug = data_aug(inputs)\n",
    "    \n",
    "    coat = coatnet.CoAtNet0(pretrained='imagenet',\n",
    "                            num_classes=512,\n",
    "                            classifier_activation='relu', \n",
    "                            input_shape=(256,256,3), \n",
    "                            drop_connect_rate = 0.1,\n",
    "                            dropout=0.4)\n",
    "    x = coat(inputs)\n",
    "#     x = layers.GlobalAveragePooling2D()(x)\n",
    "#     x = keras.layers.Dropout(0.3)(x)##0.3\n",
    "#     x = layers.Dense(512, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(0.3)(x)##0.3\n",
    "    x = keras.layers.Dense(64)(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)##0.6\n",
    "    output = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.models.Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def get_generators(\n",
    "    all_df,\n",
    "    fold,\n",
    "    img_type,\n",
    "    batch_size\n",
    "):\n",
    "    train_df = all_df[all_df.fold != fold].reset_index(drop=True)\n",
    "    test_df = all_df[all_df.fold == fold].reset_index(drop=True)\n",
    "    \n",
    "    train_generator = DataSlabGenerator(train_df, \n",
    "                                   IMAGE_PATH, \n",
    "                                   img_type = img_type,\n",
    "                                   verbose=False, \n",
    "                                   n_channels=3, \n",
    "                                   set_type='train',\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=True)\n",
    "    \n",
    "    valid_generator = DataSlabGenerator(test_df, \n",
    "                                   IMAGE_PATH, \n",
    "                                   img_type = img_type,\n",
    "                                   verbose=False, \n",
    "                                   set_type = 'valid',\n",
    "                                   n_channels=3, \n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=False)\n",
    "    \n",
    "    test_generator = DataSlabGenerator(test_df, \n",
    "                                   IMAGE_PATH, \n",
    "                                   img_type = img_type,\n",
    "                                   verbose=False, \n",
    "                                   set_type = 'test',\n",
    "                                   n_channels=3, \n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=False)\n",
    "    \n",
    "    return train_generator, valid_generator, test_generator\n",
    "\n",
    "def train(all_df, model_name, fold):\n",
    "    model = get_model()\n",
    "\n",
    "    train_generator, valid_generator, test_generator = get_generators(all_df, \n",
    "                                                                      fold, \n",
    "                                                                      IMG_TYPE, \n",
    "                                                                      BATCH_SIZE)\n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.0001) \n",
    "#                                 beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "    scheduler = keras.optimizers.schedules.CosineDecay(0.0001, 100000)\n",
    "#                                 beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "\n",
    "    scheduler = keras.callbacks.LearningRateScheduler(scheduler)\n",
    "    checkpoint_path = os.path.join(MODEL_DIR, f\"{model_name}_fold_{fold:02d}.h5\")\n",
    "    training_histories = []\n",
    "\n",
    "    check = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_path,\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "    es = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        verbose=1,\n",
    "        patience=1,\n",
    "    )\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", \n",
    "                  optimizer=opt, \n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    hist_full = model.fit(\n",
    "        train_generator,\n",
    "        epochs=EPOCHS,\n",
    "#         steps_per_epoch=10,\n",
    "#         validation_steps=10,\n",
    "        validation_data=valid_generator,\n",
    "        callbacks=[check,es,scheduler],\n",
    "    )\n",
    "\n",
    "    training_histories.append(pd.DataFrame(hist_full.history))\n",
    "\n",
    "    df_hist = pd.concat(training_histories, axis=0, ignore_index=True, sort=False)\n",
    "    df_hist.to_csv(os.path.join(MODEL_DIR, f\"{model_name}_fold_{fold:02d}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef03052f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-paige/turku_aug_funcs.py:3: DeprecationWarning: Please use `gaussian_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
      "  from scipy.ndimage.filters import gaussian_filter\n",
      "/home/jupyter-paige/turku_aug_funcs.py:4: DeprecationWarning: Please use `map_coordinates` from the `scipy.ndimage` namespace, the `scipy.ndimage.interpolation` namespace is deprecated.\n",
      "  from scipy.ndimage.interpolation import map_coordinates\n"
     ]
    }
   ],
   "source": [
    "import turku_aug_funcs\n",
    "\n",
    "class DataSlabGenerator(Sequence):\n",
    "    \"\"\"Generates data for Keras\n",
    "    Sequence based data generator. Suitable for building data generator for training and prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, all_df, image_path, img_type, set_type, n_channels=1, \n",
    "                 batch_size=32, dim=256, num_pos=None, n_classes=2, shuffle=True,\n",
    "                 verbose=False, to_fit=True,\n",
    "                 ):\n",
    "        \"\"\"Initialization\n",
    "        :param list_IDs: list of all 'label' ids to use in the generator\n",
    "        :param labels: list of image labels (file names)\n",
    "        :param image_path: path to images location\n",
    "        :param mask_path: path to masks location\n",
    "        :param to_fit: True to return X and y, False to return X only\n",
    "        :param batch_size: batch size at each iteration\n",
    "        :param dim: tuple indicating image dimension\n",
    "        :param n_channels: number of image channels\n",
    "        :param n_classes: number of output masks\n",
    "        :param shuffle: True to shuffle label indexes after every epoch\n",
    "        \"\"\"\n",
    "        self.image_path = image_path\n",
    "        self.to_fit = to_fit\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.path_dicom = image_path\n",
    "        self.verbose = verbose\n",
    "        self.img_type = img_type\n",
    "        warnings.filterwarnings(action='ignore')\n",
    "        self.all_df = all_df\n",
    "        self.set_type = set_type\n",
    "        \n",
    "        if self.set_type == 'test':\n",
    "            self.labels = self.all_df\n",
    "        elif self.set_type == 'valid':\n",
    "            pos = self.all_df[self.all_df.pe_in_slab == True]\n",
    "            neg = self.all_df[self.all_df.pe_in_slab == False].sample(n=len(pos))\n",
    "            self.labels = pd.concat([pos,neg]).sample(frac=1).reset_index(drop=True)\n",
    "        elif self.set_type == 'train':\n",
    "            pos = self.all_df[self.all_df.pe_in_slab ==True]\n",
    "            neg = self.all_df[self.all_df.pe_in_slab == False].sample(n=len(pos))\n",
    "            self.labels = pd.concat([pos,neg]).sample(frac=1).reset_index(drop=True)\n",
    "        else:\n",
    "            print('Invalid set type, must be test, valid or train')\n",
    "            return False\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(self.set_type, self.labels.fold.unique())\n",
    "        \n",
    "        self.list_IDs = np.arange(len(self.labels))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.list_IDs)\n",
    "\n",
    "    def get_df(self):\n",
    "        return self.labels\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\n",
    "        :return: number of batches per epoch\n",
    "        \"\"\"\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\n",
    "        :param index: index of the batch\n",
    "        :return: X and y when fitting. X only when predicting\n",
    "        \"\"\"\n",
    "        \n",
    "        indexes = self.list_IDs[index * self.batch_size:((index+1) * self.batch_size)]\n",
    "        X = np.zeros([self.batch_size,self.dim,self.dim,3])\n",
    "        y = np.zeros([self.batch_size, 1])\n",
    "        for i in range(0,self.batch_size):\n",
    "            X[i], y[i] = self._load_dicom(indexes[i])\n",
    "            \n",
    "        if self.verbose == True:\n",
    "            fig, ax = plt.subplots(self.batch_size, 1, figsize=[12, 12*(self.batch_size/2)])\n",
    "            for i in range(self.batch_size):\n",
    "                ax[i].imshow(X[i])\n",
    "                ax[i].axis('off')\n",
    "        \n",
    "        X = X/255\n",
    "        if self.to_fit:\n",
    "            return (X, y)\n",
    "        else:\n",
    "            return (X)\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\n",
    "        \"\"\"\n",
    "        if self.set_type == 'train':\n",
    "            pos = self.all_df[self.all_df.pe_in_slab ==True]\n",
    "            neg = self.all_df[self.all_df.pe_in_slab == False].sample(n=len(pos))\n",
    "            self.labels = pd.concat([pos,neg]).sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "        self.list_IDs = np.arange(len(self.labels))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.list_IDs)\n",
    "\n",
    "    def _load_dicom(self, index):\n",
    "        slice = self.labels.iloc[index] \n",
    "\n",
    "        frames = np.zeros((self.dim,self.dim,3))\n",
    "        frame = cv2.imread(self.image_path+slice.StudyInstanceUID + '_'+ slice.SeriesInstanceUID\n",
    "                      + '_' + slice.SOPInstanceUID +'.png')\n",
    "        frame = frame[:,:,self.img_type]\n",
    "        frames[:,:,0] = frame\n",
    "\n",
    "        frame = cv2.imread(self.image_path+slice.StudyInstanceUID + '_'+ slice.SeriesInstanceUID\n",
    "                      + '_' + slice.SOP_1 +'.png')\n",
    "        frame = frame[:,:,self.img_type]\n",
    "        frames[:,:,1] = frame    \n",
    "\n",
    "        frame = cv2.imread(self.image_path+slice.StudyInstanceUID + '_'+ slice.SeriesInstanceUID\n",
    "                      + '_' + slice.SOP_2 +'.png')\n",
    "        frame = frame[:,:,self.img_type]\n",
    "        frames[:,:,2] = frame       \n",
    "                \n",
    "        if self.set_type == 'train':\n",
    "            \n",
    "            if random.random() > 0.5:\n",
    "                frames = cv2.GaussianBlur(frames, (3, 3), 0)\n",
    "\n",
    "            if random.random() > 0.2:\n",
    "                factor = random.randint(-5,10)/100 + 1\n",
    "                if factor > 1:\n",
    "                    radius = round((1.0/factor) * self.dim/2)\n",
    "                    mini = int(self.dim/2 -radius)\n",
    "                    maxi = int(self.dim/2 +radius)\n",
    "                    tmp_frame = frames[mini:maxi, mini:maxi,:]\n",
    "                    frames = cv2.resize(tmp_frame, (self.dim, self.dim))\n",
    "                if factor < 1:\n",
    "                    diam = int((1.0/factor) * self.dim)\n",
    "                    mini = int(round(diam/2) - self.dim/2)\n",
    "                    maxi = int(self.dim/2 + round(diam/2))\n",
    "                    tmp_frame = np.zeros((diam, diam, 3))\n",
    "                    tmp_frame[mini:maxi, mini:maxi,:] = frames\n",
    "                    frames = cv2.resize(tmp_frame, (self.dim, self.dim))\n",
    "\n",
    "            angle = random.randint(-10,10)\n",
    "            if angle != 0:\n",
    "                M = cv2.getRotationMatrix2D((self.dim/2, self.dim/2), angle, 1)\n",
    "                frames = cv2.warpAffine(frames, M, (self.dim, self.dim))\n",
    "                \n",
    "            if random.random() > 0.5:\n",
    "                gaus_noise = np.random.normal(0, 1, np.shape(frames))\n",
    "                frames = frames + gaus_noise\n",
    "                frames = np.clip(frames,0, 255)\n",
    "        \n",
    "        frames = np.reshape(frames, (256,256,3))\n",
    "        \n",
    "        if self.verbose == True:\n",
    "            print(np.shape(frames))\n",
    "            plt.imshow(frames[:,:,0])\n",
    "            plt.show()\n",
    "\n",
    "        frames = frames[None, ...]\n",
    "        y = np.array([int(slice.pe_in_slab)])\n",
    "        y = y[None, ...]\n",
    "        \n",
    "        return frames, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5252d5ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EPOCHS=3\n",
    "IMG_TYPE = CROPPED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0554c8cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-paige/.local/lib/python3.9/site-packages/tensorflow_datasets/core/tf_compat.py:59: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  tf_version = distutils.version.LooseVersion(tf.__version__)\n",
      "/home/jupyter-paige/.conda/envs/tf-gpu/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-08 14:38:32.721863: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-08 14:38:34.509538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11470 MB memory:  -> device: 0, name: NVIDIA GeForce GTX TITAN X, pci bus id: 0000:06:00.0, compute capability: 5.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Load pretrained from: /home/jupyter-paige/.keras/models/coatnet0_224_imagenet.h5\n",
      "WARNING:tensorflow:Skipping loading weights for layer #102 (named stack_3_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (6, 961). Received saved weight with shape (6, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #128 (named stack_3_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #152 (named stack_3_block_3_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_3_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #176 (named stack_3_block_4_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_4_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #200 (named stack_3_block_5_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_5_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #225 (named stack_4_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 225). Received saved weight with shape (12, 169)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #251 (named stack_4_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (24, 225). Received saved weight with shape (24, 169)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/kernel:0. Weight expects shape (768, 512). Received saved weight with shape (768, 1000)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/bias:0. Weight expects shape (512,). Received saved weight with shape (1000,)\n",
      ">>>> Reload mismatched weights: 224 -> (256, 256)\n",
      ">>>> Reload layer: stack_3_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_2_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_3_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_4_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_5_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_2_mhsa_pos_emb\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " coatnet0 (Functional)       (None, 512)               22930490  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                32832     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,963,387\n",
      "Trainable params: 22,951,739\n",
      "Non-trainable params: 11,648\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-08 14:39:13.464397: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8401\n",
      "2022-12-08 14:39:15.307042: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3007/10834 [=======>......................] - ETA: 2:08:37 - loss: 0.5314 - accuracy: 0.7212"
     ]
    }
   ],
   "source": [
    "for fold in range(6,8):\n",
    "    print(fold)\n",
    "    train(all_ids_slab, 'coat-net-2-kfold', fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65300a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(2,3):\n",
    "    print(fold)\n",
    "    train(all_ids_slab, 'coat-net-diff-kfold', fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798cd9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Load pretrained from: /home/jupyter-paige/.keras/models/coatnet0_224_imagenet.h5\n",
      "WARNING:tensorflow:Skipping loading weights for layer #102 (named stack_3_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (6, 961). Received saved weight with shape (6, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #128 (named stack_3_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #152 (named stack_3_block_3_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_3_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #176 (named stack_3_block_4_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_4_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #200 (named stack_3_block_5_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_5_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #225 (named stack_4_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 225). Received saved weight with shape (12, 169)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #251 (named stack_4_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (24, 225). Received saved weight with shape (24, 169)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/kernel:0. Weight expects shape (768, 512). Received saved weight with shape (768, 1000)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/bias:0. Weight expects shape (512,). Received saved weight with shape (1000,)\n",
      ">>>> Reload mismatched weights: 224 -> (256, 256)\n",
      ">>>> Reload layer: stack_3_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_2_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_3_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_4_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_5_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_2_mhsa_pos_emb\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " coatnet0 (Functional)       (None, 512)               22930490  \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                32832     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,963,387\n",
      "Trainable params: 22,951,739\n",
      "Non-trainable params: 11,648\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/6\n",
      " 2824/10734 [======>.......................] - ETA: 53:30 - loss: 0.4920 - accuracy: 0.7450"
     ]
    }
   ],
   "source": [
    "EPOCHS=6\n",
    "IMG_TYPE = CROPPED\n",
    "for fold in range(1,10):    \n",
    "    if fold != 2:\n",
    "        train(all_ids_slab, 'coat-net-diff-kfold', fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e7376ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_817364/1642990385.py:1: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_ids = pd.read_csv('all_ids_updated.csv')\n"
     ]
    }
   ],
   "source": [
    "all_ids = pd.read_csv('all_ids_updated.csv')\n",
    "all_ids = all_ids[all_ids.contains_lung == True].sample(frac=1).reset_index(drop=True)\n",
    "all_ids.ycoord = all_ids.ycoord.replace('True', '1.0').astype('float')\n",
    "\n",
    "all_ids_small = all_ids.drop(columns=['negative_exam_for_pe', 'qa_motion',\n",
    "       'qa_contrast', 'flow_artifact', 'rv_lv_ratio_gte_1', 'rv_lv_ratio_lt_1',\n",
    "       'leftsided_pe', 'chronic_pe', 'true_filling_defect_not_pe',\n",
    "       'rightsided_pe', 'acute_and_chronic_pe', 'central_pe', 'indeterminate',\n",
    "       'contains_lung'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17f22528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Load pretrained from: /home/jupyter-paige/.keras/models/coatnet0_224_imagenet.h5\n",
      "WARNING:tensorflow:Skipping loading weights for layer #102 (named stack_3_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (6, 961). Received saved weight with shape (6, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #128 (named stack_3_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #152 (named stack_3_block_3_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_3_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #176 (named stack_3_block_4_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_4_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #200 (named stack_3_block_5_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_5_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #225 (named stack_4_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 225). Received saved weight with shape (12, 169)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #251 (named stack_4_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (24, 225). Received saved weight with shape (24, 169)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/kernel:0. Weight expects shape (768, 512). Received saved weight with shape (768, 1000)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/bias:0. Weight expects shape (512,). Received saved weight with shape (1000,)\n",
      ">>>> Reload mismatched weights: 224 -> (256, 256)\n",
      ">>>> Reload layer: stack_3_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_2_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_3_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_4_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_5_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_2_mhsa_pos_emb\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " coatnet0 (Functional)       (None, 512)               22930490  \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                32832     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,963,322\n",
      "Trainable params: 22,951,674\n",
      "Non-trainable params: 11,648\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model(3)\n",
    "model.load_weights('/home/shared/model_checkpoint_paige/singlescan-3channel/coat-net-kfold_fold_02.h5')\n",
    "# model_partial = keras.models.Sequential(model.layers[:-2])\n",
    "model_partial = keras.models.Model(inputs=model.input, outputs=[model.layers[-2].output])\n",
    "model_partial.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91185534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0ef1b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Load pretrained from: /home/jupyter-paige/.keras/models/coatnet0_224_imagenet.h5\n",
      "WARNING:tensorflow:Skipping loading weights for layer #102 (named stack_3_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (6, 961). Received saved weight with shape (6, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #128 (named stack_3_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #152 (named stack_3_block_3_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_3_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #176 (named stack_3_block_4_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_4_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #200 (named stack_3_block_5_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_5_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #225 (named stack_4_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 225). Received saved weight with shape (12, 169)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #251 (named stack_4_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (24, 225). Received saved weight with shape (24, 169)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/kernel:0. Weight expects shape (768, 512). Received saved weight with shape (768, 1000)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/bias:0. Weight expects shape (512,). Received saved weight with shape (1000,)\n",
      ">>>> Reload mismatched weights: 224 -> (256, 256)\n",
      ">>>> Reload layer: stack_3_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_2_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_3_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_4_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_5_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_2_mhsa_pos_emb\n",
      "Model: \"model_32\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_25 (InputLayer)       [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " coatnet0 (Functional)       (None, 512)               22930490  \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 64)                32832     \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,963,387\n",
      "Trainable params: 22,951,739\n",
      "Non-trainable params: 11,648\n",
      "_________________________________________________________________\n",
      "None\n",
      ">>>> Load pretrained from: /home/jupyter-paige/.keras/models/coatnet0_224_imagenet.h5\n",
      "WARNING:tensorflow:Skipping loading weights for layer #102 (named stack_3_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (6, 961). Received saved weight with shape (6, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #128 (named stack_3_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #152 (named stack_3_block_3_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_3_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #176 (named stack_3_block_4_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_4_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #200 (named stack_3_block_5_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_5_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #225 (named stack_4_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 225). Received saved weight with shape (12, 169)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #251 (named stack_4_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (24, 225). Received saved weight with shape (24, 169)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/kernel:0. Weight expects shape (768, 512). Received saved weight with shape (768, 1000)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/bias:0. Weight expects shape (512,). Received saved weight with shape (1000,)\n",
      ">>>> Reload mismatched weights: 224 -> (256, 256)\n",
      ">>>> Reload layer: stack_3_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_2_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_3_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_4_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_5_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_2_mhsa_pos_emb\n",
      "Model: \"model_34\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_27 (InputLayer)       [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " coatnet0 (Functional)       (None, 512)               22930490  \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 64)                32832     \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,963,387\n",
      "Trainable params: 22,951,739\n",
      "Non-trainable params: 11,648\n",
      "_________________________________________________________________\n",
      "None\n",
      ">>>> Load pretrained from: /home/jupyter-paige/.keras/models/coatnet0_224_imagenet.h5\n",
      "WARNING:tensorflow:Skipping loading weights for layer #102 (named stack_3_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (6, 961). Received saved weight with shape (6, 729)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping loading weights for layer #128 (named stack_3_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #152 (named stack_3_block_3_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_3_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #176 (named stack_3_block_4_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_4_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #200 (named stack_3_block_5_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_5_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #225 (named stack_4_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 225). Received saved weight with shape (12, 169)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #251 (named stack_4_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (24, 225). Received saved weight with shape (24, 169)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/kernel:0. Weight expects shape (768, 512). Received saved weight with shape (768, 1000)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/bias:0. Weight expects shape (512,). Received saved weight with shape (1000,)\n",
      ">>>> Reload mismatched weights: 224 -> (256, 256)\n",
      ">>>> Reload layer: stack_3_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_2_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_3_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_4_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_5_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_2_mhsa_pos_emb\n",
      "Model: \"model_36\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_29 (InputLayer)       [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " coatnet0 (Functional)       (None, 512)               22930490  \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 64)                32832     \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,963,387\n",
      "Trainable params: 22,951,739\n",
      "Non-trainable params: 11,648\n",
      "_________________________________________________________________\n",
      "None\n",
      ">>>> Load pretrained from: /home/jupyter-paige/.keras/models/coatnet0_224_imagenet.h5\n",
      "WARNING:tensorflow:Skipping loading weights for layer #102 (named stack_3_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (6, 961). Received saved weight with shape (6, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #128 (named stack_3_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #152 (named stack_3_block_3_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_3_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #176 (named stack_3_block_4_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_4_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #200 (named stack_3_block_5_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_5_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #225 (named stack_4_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 225). Received saved weight with shape (12, 169)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #251 (named stack_4_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (24, 225). Received saved weight with shape (24, 169)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/kernel:0. Weight expects shape (768, 512). Received saved weight with shape (768, 1000)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/bias:0. Weight expects shape (512,). Received saved weight with shape (1000,)\n",
      ">>>> Reload mismatched weights: 224 -> (256, 256)\n",
      ">>>> Reload layer: stack_3_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_2_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_3_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_4_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_5_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_2_mhsa_pos_emb\n",
      "Model: \"model_38\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_31 (InputLayer)       [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " coatnet0 (Functional)       (None, 512)               22930490  \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 64)                32832     \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,963,387\n",
      "Trainable params: 22,951,739\n",
      "Non-trainable params: 11,648\n",
      "_________________________________________________________________\n",
      "None\n",
      ">>>> Load pretrained from: /home/jupyter-paige/.keras/models/coatnet0_224_imagenet.h5\n",
      "WARNING:tensorflow:Skipping loading weights for layer #102 (named stack_3_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (6, 961). Received saved weight with shape (6, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #128 (named stack_3_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #152 (named stack_3_block_3_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_3_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping loading weights for layer #176 (named stack_3_block_4_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_4_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #200 (named stack_3_block_5_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_5_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #225 (named stack_4_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 225). Received saved weight with shape (12, 169)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #251 (named stack_4_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (24, 225). Received saved weight with shape (24, 169)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/kernel:0. Weight expects shape (768, 512). Received saved weight with shape (768, 1000)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/bias:0. Weight expects shape (512,). Received saved weight with shape (1000,)\n",
      ">>>> Reload mismatched weights: 224 -> (256, 256)\n",
      ">>>> Reload layer: stack_3_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_2_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_3_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_4_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_5_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_2_mhsa_pos_emb\n",
      "Model: \"model_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_33 (InputLayer)       [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " coatnet0 (Functional)       (None, 512)               22930490  \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 64)                32832     \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,963,387\n",
      "Trainable params: 22,951,739\n",
      "Non-trainable params: 11,648\n",
      "_________________________________________________________________\n",
      "None\n",
      ">>>> Load pretrained from: /home/jupyter-paige/.keras/models/coatnet0_224_imagenet.h5\n",
      "WARNING:tensorflow:Skipping loading weights for layer #102 (named stack_3_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (6, 961). Received saved weight with shape (6, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #128 (named stack_3_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #152 (named stack_3_block_3_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_3_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #176 (named stack_3_block_4_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_4_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #200 (named stack_3_block_5_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_5_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #225 (named stack_4_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 225). Received saved weight with shape (12, 169)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #251 (named stack_4_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (24, 225). Received saved weight with shape (24, 169)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/kernel:0. Weight expects shape (768, 512). Received saved weight with shape (768, 1000)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/bias:0. Weight expects shape (512,). Received saved weight with shape (1000,)\n",
      ">>>> Reload mismatched weights: 224 -> (256, 256)\n",
      ">>>> Reload layer: stack_3_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_2_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_3_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_4_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_5_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_2_mhsa_pos_emb\n",
      "Model: \"model_42\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_35 (InputLayer)       [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " coatnet0 (Functional)       (None, 512)               22930490  \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 64)                32832     \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,963,387\n",
      "Trainable params: 22,951,739\n",
      "Non-trainable params: 11,648\n",
      "_________________________________________________________________\n",
      "None\n",
      ">>>> Load pretrained from: /home/jupyter-paige/.keras/models/coatnet0_224_imagenet.h5\n",
      "WARNING:tensorflow:Skipping loading weights for layer #102 (named stack_3_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (6, 961). Received saved weight with shape (6, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #128 (named stack_3_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #152 (named stack_3_block_3_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_3_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #176 (named stack_3_block_4_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_4_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #200 (named stack_3_block_5_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_5_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping loading weights for layer #225 (named stack_4_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 225). Received saved weight with shape (12, 169)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #251 (named stack_4_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (24, 225). Received saved weight with shape (24, 169)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/kernel:0. Weight expects shape (768, 512). Received saved weight with shape (768, 1000)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/bias:0. Weight expects shape (512,). Received saved weight with shape (1000,)\n",
      ">>>> Reload mismatched weights: 224 -> (256, 256)\n",
      ">>>> Reload layer: stack_3_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_2_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_3_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_4_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_5_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_2_mhsa_pos_emb\n",
      "Model: \"model_44\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_37 (InputLayer)       [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " coatnet0 (Functional)       (None, 512)               22930490  \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 64)                32832     \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,963,387\n",
      "Trainable params: 22,951,739\n",
      "Non-trainable params: 11,648\n",
      "_________________________________________________________________\n",
      "None\n",
      ">>>> Load pretrained from: /home/jupyter-paige/.keras/models/coatnet0_224_imagenet.h5\n",
      "WARNING:tensorflow:Skipping loading weights for layer #102 (named stack_3_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (6, 961). Received saved weight with shape (6, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #128 (named stack_3_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #152 (named stack_3_block_3_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_3_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #176 (named stack_3_block_4_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_4_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #200 (named stack_3_block_5_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_5_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #225 (named stack_4_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 225). Received saved weight with shape (12, 169)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #251 (named stack_4_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (24, 225). Received saved weight with shape (24, 169)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/kernel:0. Weight expects shape (768, 512). Received saved weight with shape (768, 1000)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/bias:0. Weight expects shape (512,). Received saved weight with shape (1000,)\n",
      ">>>> Reload mismatched weights: 224 -> (256, 256)\n",
      ">>>> Reload layer: stack_3_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_2_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_3_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_4_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_5_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_2_mhsa_pos_emb\n",
      "Model: \"model_46\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_39 (InputLayer)       [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " coatnet0 (Functional)       (None, 512)               22930490  \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 64)                32832     \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,963,387\n",
      "Trainable params: 22,951,739\n",
      "Non-trainable params: 11,648\n",
      "_________________________________________________________________\n",
      "None\n",
      ">>>> Load pretrained from: /home/jupyter-paige/.keras/models/coatnet0_224_imagenet.h5\n",
      "WARNING:tensorflow:Skipping loading weights for layer #102 (named stack_3_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (6, 961). Received saved weight with shape (6, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #128 (named stack_3_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #152 (named stack_3_block_3_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_3_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #176 (named stack_3_block_4_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_4_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #200 (named stack_3_block_5_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_5_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #225 (named stack_4_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 225). Received saved weight with shape (12, 169)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #251 (named stack_4_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (24, 225). Received saved weight with shape (24, 169)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/kernel:0. Weight expects shape (768, 512). Received saved weight with shape (768, 1000)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/bias:0. Weight expects shape (512,). Received saved weight with shape (1000,)\n",
      ">>>> Reload mismatched weights: 224 -> (256, 256)\n",
      ">>>> Reload layer: stack_3_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_2_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_3_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_4_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_5_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_2_mhsa_pos_emb\n",
      "Model: \"model_48\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_41 (InputLayer)       [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " coatnet0 (Functional)       (None, 512)               22930490  \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 64)                32832     \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,963,387\n",
      "Trainable params: 22,951,739\n",
      "Non-trainable params: 11,648\n",
      "_________________________________________________________________\n",
      "None\n",
      ">>>> Load pretrained from: /home/jupyter-paige/.keras/models/coatnet0_224_imagenet.h5\n",
      "WARNING:tensorflow:Skipping loading weights for layer #102 (named stack_3_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (6, 961). Received saved weight with shape (6, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #128 (named stack_3_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #152 (named stack_3_block_3_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_3_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #176 (named stack_3_block_4_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_4_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #200 (named stack_3_block_5_mhsa_pos_emb) due to mismatch in shape for weight stack_3_block_5_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 961). Received saved weight with shape (12, 729)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #225 (named stack_4_block_1_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_1_mhsa_pos_emb/positional_embedding:0. Weight expects shape (12, 225). Received saved weight with shape (12, 169)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #251 (named stack_4_block_2_mhsa_pos_emb) due to mismatch in shape for weight stack_4_block_2_mhsa_pos_emb/positional_embedding:0. Weight expects shape (24, 225). Received saved weight with shape (24, 169)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/kernel:0. Weight expects shape (768, 512). Received saved weight with shape (768, 1000)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #266 (named predictions) due to mismatch in shape for weight predictions/bias:0. Weight expects shape (512,). Received saved weight with shape (1000,)\n",
      ">>>> Reload mismatched weights: 224 -> (256, 256)\n",
      ">>>> Reload layer: stack_3_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_2_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_3_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_4_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_3_block_5_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_1_mhsa_pos_emb\n",
      ">>>> Reload layer: stack_4_block_2_mhsa_pos_emb\n",
      "Model: \"model_50\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_43 (InputLayer)       [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " coatnet0 (Functional)       (None, 512)               22930490  \n",
      "                                                                 \n",
      " dropout_42 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 64)                32832     \n",
      "                                                                 \n",
      " dropout_43 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,963,387\n",
      "Trainable params: 22,951,739\n",
      "Non-trainable params: 11,648\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "model_partials = []\n",
    "model_parts = []\n",
    "for i in range(0,10):\n",
    "    models.append(get_model())\n",
    "    models[i].load_weights(MODEL_DIR + 'coat-net-2-kfold_fold_0'+ str(i) +'.h5')\n",
    "    model_partials.append(keras.models.Model(inputs=models[i].input, outputs=[models[i].layers[-3].output]))\n",
    "#     model_parts.append(keras.models.Model(inputs=models[i].input, outputs=[models[i].layers[-4].output]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c464d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids_slab_small = all_ids_slab.drop(columns=['qa_motion',\n",
    "       'qa_contrast', 'flow_artifact', 'rv_lv_ratio_gte_1', 'rv_lv_ratio_lt_1',\n",
    "       'leftsided_pe', 'chronic_pe', 'true_filling_defect_not_pe',\n",
    "       'rightsided_pe', 'acute_and_chronic_pe', 'central_pe', 'indeterminate',\n",
    "       'contains_lung', 'endStudyUID', 'pe_1', 'pe_2',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b523e7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello?\n",
      "5561\n"
     ]
    }
   ],
   "source": [
    "directory = '/home/shared/nps/coat_2_00/'\n",
    "\n",
    "lisdir = os.listdir(directory)\n",
    "print('hello?')\n",
    "print(len(lisdir))\n",
    "\n",
    "files = pd.DataFrame({'file_name':lisdir})\n",
    "files['StudyInstanceUID'] = files['file_name'].str.replace('.npy','')\n",
    "\n",
    "remaining = all_ids_slab[~all_ids_slab.StudyInstanceUID.isin(files.StudyInstanceUID.unique())]\n",
    "remaining = remaining.sort_values(by='StudyInstanceUID', ascending=False)\n",
    "\n",
    "unique_ids = remaining.StudyInstanceUID.unique()[:600]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c94259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 time is:  2022-12-10 12:03:17.464163\n",
      "10 time is:  2022-12-10 12:10:51.079062\n",
      "20 time is:  2022-12-10 12:18:08.251402\n",
      "30 time is:  2022-12-10 12:24:05.074339\n",
      "40 time is:  2022-12-10 12:30:13.503649\n",
      "50 time is:  2022-12-10 12:37:33.751026\n",
      "60 time is:  2022-12-10 12:44:32.096984\n",
      "70 time is:  2022-12-10 12:51:40.803704\n",
      "80 time is:  2022-12-10 12:58:44.706362\n",
      "90 time is:  2022-12-10 13:05:18.434465\n",
      "100 time is:  2022-12-10 13:11:52.986013\n",
      "110 time is:  2022-12-10 13:17:58.081326\n",
      "120 time is:  2022-12-10 13:24:18.855944\n",
      "130 time is:  2022-12-10 13:31:20.379284\n",
      "140 time is:  2022-12-10 13:37:30.124312\n",
      "150 time is:  2022-12-10 13:44:06.336379\n",
      "160 time is:  2022-12-10 13:51:31.224850\n",
      "170 time is:  2022-12-10 13:58:32.214167\n",
      "180 time is:  2022-12-10 14:04:53.171459\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import gc\n",
    "\n",
    "all_ids_slab = all_ids_slab.sort_values(by='StudyInstanceUID')\n",
    "unique_ids = all_ids_slab.StudyInstanceUID.unique()\n",
    "unique_ids = unique_ids[int(np.floor(len(unique_ids)*0.7)):]\n",
    "\n",
    "IMG_TYPE = CROPPED\n",
    "IMAGE_PATH = '/home/shared/nps/imgs/'\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "for i in range(0,len(unique_ids)):\n",
    "    exam_id = unique_ids[i]\n",
    "    exam = all_ids_slab_small[all_ids_slab_small.StudyInstanceUID == exam_id]\n",
    "    exam = exam.sort_values(by='ycoord')\n",
    "    exam_generator = DataSlabGenerator(exam, \n",
    "                                   IMAGE_PATH, \n",
    "                                   img_type = IMG_TYPE,\n",
    "                                   set_type='test',\n",
    "                                   verbose=False, \n",
    "                                   shuffle=False,\n",
    "                                   n_channels=3, \n",
    "                                   batch_size=1)\n",
    "    x_test = np.zeros([len(exam),256,256,3])\n",
    "    for j in range(0,len(exam)):\n",
    "        x_test[j],_ = exam_generator.__getitem__(j)\n",
    "           \n",
    "    feats = np.zeros([10, len(exam),64])\n",
    "#     feats_big = np.zeros([10,len(exam),512])\n",
    "    results = np.zeros([10,len(exam),1])\n",
    "    for j in range(int(len(exam)/BATCH_SIZE)):\n",
    "        for k in range(0,10):\n",
    "            feats[k, j*BATCH_SIZE:(j+1)*BATCH_SIZE] = np.array(model_partials[k](x_test[j*BATCH_SIZE:(j+1)*BATCH_SIZE]))\n",
    "            results[k, j*BATCH_SIZE:(j+1)*BATCH_SIZE] = np.array(models[k](x_test[j*BATCH_SIZE:(j+1)*BATCH_SIZE]))\n",
    "#             feats_big[k, j*BATCH_SIZE:(j+1)*BATCH_SIZE] = np.array(model_parts[k](x_test[j*BATCH_SIZE:(j+1)*BATCH_SIZE]))\n",
    "    if len(exam)%BATCH_SIZE != 0:\n",
    "        for k in range(0,10):\n",
    "            feats[k, (j+1)*BATCH_SIZE:] = np.array(model_partials[k](x_test[(j+1)*BATCH_SIZE:]))\n",
    "            results[k, (j+1)*BATCH_SIZE:] = np.array(models[k](x_test[(j+1)*BATCH_SIZE:]))\n",
    "#             feats_big[k, (j+1)*BATCH_SIZE:] = np.array(model_parts[k](x_test[(j+1)*BATCH_SIZE:]))\n",
    "        \n",
    "    for k in range(0,10):\n",
    "        exam['features'] = feats[k].tolist()\n",
    "#         exam['features_back'] = feats_big[k].tolist()\n",
    "        exam['preds'] = results[k].tolist()\n",
    "        np.save('/home/shared/nps/coat_2_0'+ str(k)+ '/' + exam_id, exam.to_dict(orient='records'))\n",
    "            \n",
    "    if i %10 == 0:\n",
    "        print(i, 'time is: ', datetime.datetime.now())\n",
    "        gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
