{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34dbfb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-paige/.conda/envs/tf-gpu/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.1\n",
      "2.8.0\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers\n",
    "# !pip install joblib\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as ptchs\n",
    "import cv2\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import ConvNextFeatureExtractor, TFConvNextModel, ConvNextConfig, ConvNextModel\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow.keras import Model, Sequential \n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, AveragePooling2D, GlobalAveragePooling1D, GlobalAveragePooling2D, MaxPooling2D , Conv3D, Layer, MaxPooling2D, Dropout, Flatten, Dense, GRU, ConvLSTM2D, Input, BatchNormalization, TimeDistributed, MaxPooling3D, Bidirectional, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from random import shuffle\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from keras.constraints import max_norm\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "# import bisect\n",
    "np.random.seed(8)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "MAX_SEQ_LENGTH = 200\n",
    "NUM_FEATURES = 1024\n",
    "IMG_SIZE = 512\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 2\n",
    "NUM_SCANS = 8\n",
    "NUM_CHANNELS = 1\n",
    "INPUT_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1ee1969",
   "metadata": {},
   "outputs": [],
   "source": [
    "CROPPED = 0\n",
    "MASKED = 1\n",
    "ORIGINAL = 2\n",
    "IMAGE_PATH = '/home/shared/nps/imgs/'\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc226899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4138698/1656503385.py:1: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_ids = pd.read_csv('all_ids_updated.csv')\n"
     ]
    }
   ],
   "source": [
    "all_ids = pd.read_csv('all_ids_updated.csv')\n",
    "all_ids = all_ids[all_ids.contains_lung == True].sample(frac=1).reset_index(drop=True)\n",
    "all_ids.ycoord = all_ids.ycoord.replace('True', '1.0').astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dfd25a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_df_upd.csv').drop(columns='Unnamed: 0')\n",
    "test_df = pd.read_csv('test_df_upd.csv').drop(columns='Unnamed: 0')\n",
    "val_df = pd.read_csv('val_df_upd.csv').drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2090b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, val_df])\n",
    "train_all_ids = all_ids[all_ids.StudyInstanceUID.isin(train_df.StudyInstanceUID)]\n",
    "study_ids = train_all_ids.pivot_table(index='StudyInstanceUID').sort_values('pe_present_on_image').index\n",
    "\n",
    "fold_0_ids = [study_ids[i] for i in range(len(study_ids)) if i%5 == 0]\n",
    "fold_1_ids = [study_ids[i] for i in range(len(study_ids)) if i%5 == 1]\n",
    "fold_2_ids = [study_ids[i] for i in range(len(study_ids)) if i%5 == 2]\n",
    "fold_3_ids = [study_ids[i] for i in range(len(study_ids)) if i%5 == 3]\n",
    "fold_4_ids = [study_ids[i] for i in range(len(study_ids)) if i%5 == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b825178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95746 95744\n",
      "86853 8893\n"
     ]
    }
   ],
   "source": [
    "all_ids.ycoord = all_ids.ycoord.astype('float')\n",
    "\n",
    "SLAB_SIZE = 3\n",
    "\n",
    "all_ids_slab = all_ids.sort_values(['StudyInstanceUID', 'ycoord'])\n",
    "all_ids_slab['endStudyUID'] = all_ids_slab.StudyInstanceUID.shift(SLAB_SIZE - 1)\n",
    "\n",
    "all_ids_slab['pe_in_slab'] = all_ids_slab.pe_present_on_image\n",
    "for i in range(1, SLAB_SIZE):\n",
    "    all_ids_slab['pe_' + str(i)] = all_ids_slab.pe_present_on_image.shift(i)\n",
    "    all_ids_slab['SOP_' + str(i)] = all_ids_slab.SOPInstanceUID.shift(i)\n",
    "    all_ids_slab['pe_in_slab'] = all_ids_slab['pe_in_slab'] + all_ids_slab['pe_' + str(i)]\n",
    "\n",
    "all_ids_slab['pe_in_slab'] = all_ids_slab['pe_1']\n",
    "\n",
    "all_ids_slab = all_ids_slab[all_ids_slab.StudyInstanceUID == all_ids_slab.endStudyUID]\n",
    "\n",
    "all_ids_slab = all_ids_slab.reset_index(drop=True)\n",
    "print(len(all_ids_slab[all_ids_slab.pe_in_slab ==True]), len(all_ids_slab[all_ids_slab.pe_present_on_image == True]))\n",
    "\n",
    "train_slab = all_ids_slab[all_ids_slab.StudyInstanceUID.isin(train_df.StudyInstanceUID)]\n",
    "test_slab = all_ids_slab[all_ids_slab.StudyInstanceUID.isin(test_df.StudyInstanceUID)]\n",
    "print(len(train_slab[train_slab.pe_in_slab ==True]), len(test_slab[test_slab.pe_in_slab == True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8ed0da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4138698/108160448.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_slab['4fold'] = train_slab.StudyInstanceUID.isin(fold_0_ids)\n",
      "/tmp/ipykernel_4138698/108160448.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_slab['3fold'] = train_slab.StudyInstanceUID.isin(fold_1_ids)\n",
      "/tmp/ipykernel_4138698/108160448.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_slab['2fold'] = train_slab.StudyInstanceUID.isin(fold_2_ids)\n",
      "/tmp/ipykernel_4138698/108160448.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_slab['1fold'] = train_slab.StudyInstanceUID.isin(fold_3_ids)\n",
      "/tmp/ipykernel_4138698/108160448.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_slab['0fold'] = train_slab.StudyInstanceUID.isin(fold_4_ids)\n"
     ]
    }
   ],
   "source": [
    "train_slab['4fold'] = train_slab.StudyInstanceUID.isin(fold_0_ids)\n",
    "train_slab['3fold'] = train_slab.StudyInstanceUID.isin(fold_1_ids)\n",
    "train_slab['2fold'] = train_slab.StudyInstanceUID.isin(fold_2_ids)\n",
    "train_slab['1fold'] = train_slab.StudyInstanceUID.isin(fold_3_ids)\n",
    "train_slab['0fold'] = train_slab.StudyInstanceUID.isin(fold_4_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af3a43bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17096.0\n",
      "17544.0\n",
      "17197.0\n",
      "17418.0\n",
      "17598.0\n"
     ]
    }
   ],
   "source": [
    "print(train_slab[train_slab['4fold'] == True].pe_in_slab.sum())\n",
    "print(train_slab[train_slab['3fold'] == True].pe_in_slab.sum())\n",
    "print(train_slab[train_slab['2fold'] == True].pe_in_slab.sum())\n",
    "print(train_slab[train_slab['1fold'] == True].pe_in_slab.sum())\n",
    "print(train_slab[train_slab['0fold'] == True].pe_in_slab.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83698325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos for training: 1047595\n",
      "Total videos for testing: 113175\n",
      "resampling\n",
      "0fold\n",
      "resampling\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Total videos for training: {len(train_slab)}\")\n",
    "print(f\"Total videos for testing: {len(test_slab)}\")\n",
    "\n",
    "\n",
    "class DataSlabGenerator(Sequence):\n",
    "    \"\"\"Generates data for Keras\n",
    "    Sequence based data generator. Suitable for building data generator for training and prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, list_IDs, labels, image_path, img_type,\n",
    "                 n_channels=1, to_fit=True, batch_size=32, dim=256, num_pos=None,\n",
    "                 n_classes=2, shuffle=True, verbose=False, set_type='valid', \n",
    "                 convex_hull = False, full_set = None, resample = False):\n",
    "        \"\"\"Initialization\n",
    "        :param list_IDs: list of all 'label' ids to use in the generator\n",
    "        :param labels: list of image labels (file names)\n",
    "        :param image_path: path to images location\n",
    "        :param mask_path: path to masks location\n",
    "        :param to_fit: True to return X and y, False to return X only\n",
    "        :param batch_size: batch size at each iteration\n",
    "        :param dim: tuple indicating image dimension\n",
    "        :param n_channels: number of image channels\n",
    "        :param n_classes: number of output masks\n",
    "        :param shuffle: True to shuffle label indexes after every epoch\n",
    "        \"\"\"\n",
    "        self.list_IDs = list_IDs\n",
    "        self.labels = labels\n",
    "        self.image_path = image_path\n",
    "        self.to_fit = to_fit\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.path_dicom = image_path\n",
    "        self.verbose = verbose\n",
    "        self.img_type = img_type\n",
    "        warnings.filterwarnings(action='ignore')\n",
    "        self.full_set = full_set\n",
    "        self.resample = resample\n",
    "        self.epoch = 0\n",
    "        self.set_type = set_type\n",
    "        self.num_pos = num_pos\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\n",
    "        :return: number of batches per epoch\n",
    "        \"\"\"\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\n",
    "        :param index: index of the batch\n",
    "        :return: X and y when fitting. X only when predicting\n",
    "        \"\"\"\n",
    "        \n",
    "        indexes = self.list_IDs[index * self.batch_size:((index+1) * self.batch_size)]\n",
    "        X = np.zeros([self.batch_size,self.dim,self.dim,3])\n",
    "        y = np.zeros([self.batch_size, 1])\n",
    "        for i in range(0,self.batch_size):\n",
    "            X[i], y[i] = self._load_dicom(indexes[i])\n",
    "            \n",
    "        if self.verbose == True:\n",
    "            fig, ax = plt.subplots(self.batch_size, 1, figsize=[12, 12*(self.batch_size/2)])\n",
    "            for i in range(self.batch_size):\n",
    "                ax[i].imshow(X[i])\n",
    "                ax[i].axis('off')\n",
    "        \n",
    "        X = X/125.5 -1\n",
    "            \n",
    "        if self.to_fit:\n",
    "            return (X, y)\n",
    "        else:\n",
    "            return (X)\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\n",
    "        \"\"\"\n",
    "        if self.resample == True:\n",
    "            print('resampling')\n",
    "            if self.set_type == 'valid':\n",
    "                sample_set = self.full_set[self.full_set[str(self.epoch%5)+'fold'] == True]\n",
    "            if self.set_type == 'train':\n",
    "                sample_set = self.full_set[self.full_set[str(self.epoch%5)+'fold'] == False]\n",
    "                print(str(self.epoch%5)+'fold')\n",
    "            sample_set = sample_set.reset_index(drop=True)\n",
    "            set_pos = sample_set[sample_set.pe_in_slab ==True]\n",
    "            set_neg = sample_set[sample_set.pe_in_slab == False].sample(n=len(set_pos))\n",
    "            self.labels = pd.concat([set_pos,set_neg]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "            self.epoch = self.epoch + 1\n",
    "            \n",
    "        self.list_IDs = np.arange(len(self.labels))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.list_IDs)\n",
    "\n",
    "    def _load_dicom(self, index):\n",
    "        slice = self.labels.iloc[index] \n",
    "\n",
    "        frames = np.zeros((self.dim,self.dim,3))\n",
    "        frame = cv2.imread(self.image_path+slice.StudyInstanceUID + '_'+ slice.SeriesInstanceUID\n",
    "                      + '_' + slice.SOPInstanceUID +'.png')\n",
    "        frame = frame[:,:,self.img_type]\n",
    "        frames[:,:,0] = frame\n",
    "\n",
    "        frame = cv2.imread(self.image_path+slice.StudyInstanceUID + '_'+ slice.SeriesInstanceUID\n",
    "                      + '_' + slice.SOP_1 +'.png')\n",
    "        frame = frame[:,:,self.img_type]\n",
    "        frames[:,:,1] = frame    \n",
    "\n",
    "        frame = cv2.imread(self.image_path+slice.StudyInstanceUID + '_'+ slice.SeriesInstanceUID\n",
    "                      + '_' + slice.SOP_2 +'.png')\n",
    "        frame = frame[:,:,self.img_type]\n",
    "        frames[:,:,2] = frame   \n",
    "\n",
    "        if self.verbose == True:\n",
    "            print(np.shape(frame))\n",
    "            plt.imshow(frame)\n",
    "            plt.show()\n",
    "\n",
    "        frames = frames[None, ...]\n",
    "        y = np.array([int(slice.pe_in_slab)])\n",
    "        y = y[None, ...]\n",
    "\n",
    "        return frames, y\n",
    "        \n",
    "BATCH_SIZE = 64\n",
    "IMG_TYPE = ORIGINAL\n",
    "\n",
    "    \n",
    "training_slab_generator = DataSlabGenerator([],\n",
    "                                   train_slab, \n",
    "                                   IMAGE_PATH, \n",
    "                                   img_type = IMG_TYPE,\n",
    "                                   verbose=False, \n",
    "                                   resample=True, \n",
    "                                   full_set=train_slab,\n",
    "                                   n_channels=3, \n",
    "                                   set_type='train',\n",
    "                                   batch_size=BATCH_SIZE)\n",
    "validation_slab_generator = DataSlabGenerator([],\n",
    "                                   train_slab, \n",
    "                                   IMAGE_PATH, \n",
    "                                   img_type = IMG_TYPE,\n",
    "                                   verbose=False, \n",
    "                                   resample=True,\n",
    "                                   set_type = 'valid',\n",
    "                                   full_set = train_slab,\n",
    "                                   n_channels=3, \n",
    "                                   batch_size=BATCH_SIZE)\n",
    "test_slab_generator = DataSlabGenerator(np.arange(0, len(test_slab)),\n",
    "                                   test_slab, \n",
    "                                   IMAGE_PATH, \n",
    "                                   img_type = IMG_TYPE,\n",
    "                                   verbose=False, \n",
    "                                   resample=False, \n",
    "                                   n_channels=3, \n",
    "                                   batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b445770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-03 12:22:27.997681: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-03 12:22:29.552496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11194 MB memory:  -> device: 0, name: NVIDIA GeForce GTX TITAN X, pci bus id: 0000:06:00.0, compute capability: 5.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Load pretrained from: /home/jupyter-paige/.keras/models/resnest50_imagenet.h5\n",
      "WARNING:tensorflow:Skipping loading weights for layer #481 (named predictions) due to mismatch in shape for weight predictions/kernel:0. Weight expects shape (2048, 1). Received saved weight with shape (2048, 1000)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #481 (named predictions) due to mismatch in shape for weight predictions/bias:0. Weight expects shape (1,). Received saved weight with shape (1000,)\n"
     ]
    }
   ],
   "source": [
    "from keras_cv_attention_models import resnest\n",
    "res = resnest.ResNest50(pretrained='imagenet', num_classes=1, input_shape=(448,448,3), classifier_activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51b6f509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " data_augmentation (Sequenti  (None, 448, 448, 3)      0         \n",
      " al)                                                             \n",
      "                                                                 \n",
      " resnest50 (Functional)      (None, 1)                 25500865  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,500,865\n",
      "Trainable params: 25,436,289\n",
      "Non-trainable params: 64,576\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not interpret optimizer identifier: <keras.optimizer_v2.learning_rate_schedule.CosineDecay object at 0x7f1e70e81000>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 16\u001b[0m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mModel(inputs\u001b[38;5;241m=\u001b[39minputs, outputs\u001b[38;5;241m=\u001b[39moutputs)\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m---> 16\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedules\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCosineDecay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcosinedecay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_learning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBinaryCrossentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi-pan\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     23\u001b[0m checkpoint_filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/shared/model_checkpoint_paige/singlescan-3channel/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m filename\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.10/site-packages/keras/optimizers.py:144\u001b[0m, in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m    142\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m deserialize(config)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    145\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not interpret optimizer identifier: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(identifier))\n",
      "\u001b[0;31mValueError\u001b[0m: Could not interpret optimizer identifier: <keras.optimizer_v2.learning_rate_schedule.CosineDecay object at 0x7f1e70e81000>"
     ]
    }
   ],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "                    [\n",
    "                        layers.Resizing(512,512),\n",
    "                        layers.RandomCrop(448, 448),\n",
    "                        layers.Resizing(448,448)\n",
    "                    ],\n",
    "                    name=\"data_augmentation\",)\n",
    "inputs = layers.Input(shape=(256,256,3))\n",
    "augmented = data_augmentation(inputs)\n",
    "outputs = res(augmented)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.schedules(learning_rate=0.001, beta_1=0.9, beta_2=0.999, decay=0.01),\n",
    "    loss=tf.losses.BinaryCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "filename = 'i-pan'\n",
    "checkpoint_filepath = '/home/shared/model_checkpoint_paige/singlescan-3channel/' + filename\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    training_slab_generator,\n",
    "    validation_data = validation_slab_generator,\n",
    "    epochs = 5,\n",
    "    callbacks=[checkpoint,\n",
    "              ],\n",
    "    shuffle=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a60512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_23 (InputLayer)       [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " data_augmentation (Sequenti  (None, 256, 256, 3)      0         \n",
      " al)                                                             \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 512)               1049088   \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,614,401\n",
      "Trainable params: 24,568,961\n",
      "Non-trainable params: 45,440\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "17343/17343 [==============================] - ETA: 0s - loss: 0.4745 - accuracy: 0.7579resampling\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.45640, saving model to /home/shared/model_checkpoint_paige/singlescan-3channel/resnet-2\n",
      "17343/17343 [==============================] - 4091s 236ms/step - loss: 0.4745 - accuracy: 0.7579 - val_loss: 0.4564 - val_accuracy: 0.7695\n",
      "resampling\n",
      "2fold\n",
      "Epoch 2/10\n",
      " 7999/17343 [============>.................] - ETA: 35:22 - loss: 0.4119 - accuracy: 0.7991"
     ]
    }
   ],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "                    [\n",
    "                        layers.RandomRotation(factor=0.1, fill_mode='constant'),\n",
    "                        layers.RandomCrop(240, 240),\n",
    "                        layers.Resizing(256,256)\n",
    "                    ],\n",
    "                    name=\"data_augmentation\",)\n",
    "inputs = layers.Input(shape=(256,256,3))\n",
    "augmented = data_augmentation(inputs)\n",
    "res = keras.applications.resnet_v2.ResNet50V2(\n",
    "                        include_top=False,\n",
    "                        weights='imagenet',\n",
    "#                         input_tensor=augmented,\n",
    "                        pooling='avg',)\n",
    "x = res(augmented)\n",
    "x = layers.Dense(512)(x)\n",
    "x = layers.Dropout(0.8)(x)    \n",
    "outputs = Dense(1, activation = 'sigmoid')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, decay=0.01),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "filename = 'resnet-2'\n",
    "checkpoint_filepath = '/home/shared/model_checkpoint_paige/singlescan-3channel/' + filename\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    training_slab_generator,\n",
    "    validation_data = validation_slab_generator,\n",
    "    epochs = 10,\n",
    "    callbacks=[checkpoint,\n",
    "              ],\n",
    "    shuffle=True,\n",
    "#     steps_per_epoch=15935\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40b2a9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_39 (InputLayer)       [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " data_augmentation (Sequenti  (None, 256, 256, 3)      0         \n",
      " al)                                                             \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 512)               1049088   \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,614,401\n",
      "Trainable params: 24,568,961\n",
      "Non-trainable params: 45,440\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      " 1957/17308 [==>...........................] - ETA: 59:00 - loss: 0.6879 - accuracy: 0.5894"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [39], line 35\u001b[0m\n\u001b[1;32m     30\u001b[0m checkpoint_filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/shared/model_checkpoint_paige/singlescan-3channel/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m filename\n\u001b[1;32m     31\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[1;32m     32\u001b[0m     checkpoint_filepath, save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     33\u001b[0m )\n\u001b[0;32m---> 35\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_slab_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidation_slab_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m              \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;43;03m#     steps_per_epoch=15935\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.10/site-packages/keras/engine/training.py:1389\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1387\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[1;32m   1388\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1389\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1391\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.10/site-packages/keras/callbacks.py:438\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 438\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.10/site-packages/keras/callbacks.py:297\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 297\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    300\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.10/site-packages/keras/callbacks.py:318\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    315\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    316\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 318\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    321\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.10/site-packages/keras/callbacks.py:356\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    355\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 356\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    359\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.10/site-packages/keras/callbacks.py:1034\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1034\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.10/site-packages/keras/callbacks.py:1106\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1102\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1105\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1106\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1107\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.10/site-packages/keras/utils/tf_utils.py:563\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[1;32m    561\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m--> 563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.10/site-packages/tensorflow/python/util/nest.py:914\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    910\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    911\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    915\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.10/site-packages/tensorflow/python/util/nest.py:914\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    910\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    911\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    915\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.10/site-packages/keras/utils/tf_utils.py:557\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    555\u001b[0m   \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    556\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 557\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m   \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1223\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1201\u001b[0m \n\u001b[1;32m   1202\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1223\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1189\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1188\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1190\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "                    [\n",
    "                        layers.RandomRotation(factor=0.2, fill_mode='constant'),\n",
    "                        layers.RandomCrop(240, 240),\n",
    "                        layers.Resizing(256,256)\n",
    "                    ],\n",
    "                    name=\"data_augmentation\",)\n",
    "inputs = layers.Input(shape=(256,256,3))\n",
    "augmented = data_augmentation(inputs)\n",
    "res = keras.applications.resnet_v2.ResNet50V2(\n",
    "                        include_top=False,\n",
    "                        weights='imagenet',\n",
    "#                         input_tensor=augmented,\n",
    "                        pooling='avg',)\n",
    "x = res(augmented)\n",
    "x = layers.Dense(512)(x)\n",
    "x = layers.Dropout(0.8)(x)    \n",
    "outputs = Dense(1, activation = 'sigmoid')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, decay=0.01),\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "filename = 'resnet-2-2'\n",
    "checkpoint_filepath = '/home/shared/model_checkpoint_paige/singlescan-3channel/' + filename\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    training_slab_generator,\n",
    "    validation_data = validation_slab_generator,\n",
    "    epochs = 10,\n",
    "    callbacks=[checkpoint,\n",
    "              ],\n",
    "    shuffle=True,\n",
    "#     steps_per_epoch=15935\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e149cb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Load pretrained from: /home/jupyter-paige/.keras/models/davit_t_imagenet.h5\n",
      "WARNING:tensorflow:Skipping loading weights for layer #396 (named predictions) due to mismatch in shape for weight predictions/kernel:0. Weight expects shape (768, 1). Received saved weight with shape (768, 1000)\n",
      "WARNING:tensorflow:Skipping loading weights for layer #396 (named predictions) due to mismatch in shape for weight predictions/bias:0. Weight expects shape (1,). Received saved weight with shape (1000,)\n",
      "Model: \"model_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_50 (InputLayer)       [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " data_augmentation (Sequenti  (None, 256, 256, 3)      0         \n",
      " al)                                                             \n",
      "                                                                 \n",
      " davit_t (Functional)        (None, 1)                 27591937  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,591,937\n",
      "Trainable params: 27,591,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      " 1955/17308 [==>...........................] - ETA: 3:16:56 - loss: 0.5933 - accuracy: 0.6721"
     ]
    }
   ],
   "source": [
    "from keras_cv_attention_models import davit\n",
    "davit_t = davit.DaViT_T(pretrained='imagenet', num_classes=1, classifier_activation='sigmoid', dropout=0.8, drop_connect_rate=0.2, input_shape=(256,256,3))\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "                    [\n",
    "                        layers.RandomRotation(factor=0.2, fill_mode='constant'),\n",
    "                        layers.RandomCrop(240, 240),\n",
    "                        layers.Resizing(256,256)\n",
    "                    ],\n",
    "                    name=\"data_augmentation\",)\n",
    "inputs = layers.Input(shape=(256,256,3))\n",
    "augmented = data_augmentation(inputs)\n",
    "outputs = davit_t(augmented)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, decay=0.01),\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "filename = 'davit_t'\n",
    "checkpoint_filepath = '/home/shared/model_checkpoint_paige/singlescan-3channel/' + filename\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    training_slab_generator,\n",
    "    validation_data = validation_slab_generator,\n",
    "    epochs = 5,\n",
    "    callbacks=[checkpoint,\n",
    "              ],\n",
    "    shuffle=True,\n",
    "#     steps_per_epoch=15935\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761a94b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "davit_s = davit.DaViT_S(pretrained='imagenet', num_classes=1, classifier_activation='sigmoid', dropout=0.8, drop_connect_rate=0.2, input_shape=(256,256,3))\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "                    [\n",
    "                        layers.RandomRotation(factor=0.2, fill_mode='constant'),\n",
    "                        layers.RandomCrop(240, 240),\n",
    "                        layers.Resizing(256,256)\n",
    "                    ],\n",
    "                    name=\"data_augmentation\",)\n",
    "inputs = layers.Input(shape=(256,256,3))\n",
    "augmented = data_augmentation(inputs)\n",
    "outputs = davit_s(augmented)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, decay=0.01),\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "filename = 'davit_s'\n",
    "checkpoint_filepath = '/home/shared/model_checkpoint_paige/singlescan-3channel/' + filename\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    training_slab_generator,\n",
    "    validation_data = validation_slab_generator,\n",
    "    epochs = 5,\n",
    "    callbacks=[checkpoint,\n",
    "              ],\n",
    "    shuffle=True,\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
